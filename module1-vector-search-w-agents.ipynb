{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079feedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69885cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.11.4)\n",
      "Requirement already satisfied: markdown in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (3.8.2)\n",
      "Requirement already satisfied: reportlab in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (4.4.3)\n",
      "Requirement already satisfied: langchain in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (0.3.1)\n",
      "Requirement already satisfied: langchain-neo4j in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.5.0)\n",
      "Requirement already satisfied: langgraph in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (0.6.6)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (4.67.1)\n",
      "Requirement already satisfied: pypdf in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (5.9.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (2.2.3)\n",
      "Requirement already satisfied: neo4j in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (5.28.2)\n",
      "Requirement already satisfied: google-adk>=1.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (1.10.0)\n",
      "Requirement already satisfied: litellm in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (1.75.3)\n",
      "Requirement already satisfied: uv in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (0.8.12)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (5.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from reportlab->-r requirements.txt (line 3)) (11.3.0)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from reportlab->-r requirements.txt (line 3)) (3.4.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 4)) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 4)) (0.3.9)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 4)) (0.3.45)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 4)) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 4)) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 4)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 4)) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 4)) (3.2.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain-openai->-r requirements.txt (line 5)) (1.102.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain-openai->-r requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai->-r requirements.txt (line 5)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai->-r requirements.txt (line 5)) (2025.7.34)\n",
      "Requirement already satisfied: neo4j-graphrag<2.0.0,>=1.9.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langchain-neo4j->-r requirements.txt (line 6)) (1.9.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from neo4j->-r requirements.txt (line 12)) (2025.2)\n",
      "Requirement already satisfied: fsspec<2025.0.0,>=2024.9.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j->-r requirements.txt (line 6)) (2024.12.0)\n",
      "Requirement already satisfied: json-repair<0.45.0,>=0.44.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j->-r requirements.txt (line 6)) (0.44.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=2.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j->-r requirements.txt (line 6)) (2.2.5)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.13.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j->-r requirements.txt (line 6)) (1.16.1)\n",
      "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j->-r requirements.txt (line 6)) (6.0.12.20250809)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langgraph->-r requirements.txt (line 7)) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langgraph->-r requirements.txt (line 7)) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langgraph->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langgraph->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph->-r requirements.txt (line 7)) (1.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from tqdm->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 11)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: absolufy-imports>=0.3.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (0.3.1)\n",
      "Requirement already satisfied: authlib>=1.5.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (1.6.1)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (8.1.8)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (0.115.12)\n",
      "Requirement already satisfied: google-api-python-client>=2.157.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (2.179.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform>=1.95.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.110.0)\n",
      "Requirement already satisfied: google-cloud-secret-manager>=2.22.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (2.24.0)\n",
      "Requirement already satisfied: google-cloud-speech>=2.30.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (2.33.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (2.19.0)\n",
      "Requirement already satisfied: google-genai>=1.21.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (1.31.0)\n",
      "Requirement already satisfied: graphviz>=0.20.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (0.21)\n",
      "Requirement already satisfied: mcp>=1.8.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (1.12.4)\n",
      "Requirement already satisfied: opentelemetry-api>=1.31.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-gcp-trace>=1.9.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (1.9.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.31.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (1.36.0)\n",
      "Requirement already satisfied: starlette>=0.46.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (0.46.2)\n",
      "Requirement already satisfied: tzlocal>=5.3 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (5.3.1)\n",
      "Collecting uvicorn>=0.34.0 (from google-adk>=1.0.0->-r requirements.txt (line 13))\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: watchdog>=6.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-adk>=1.0.0->-r requirements.txt (line 13)) (6.0.0)\n",
      "Collecting websockets>=15.0.1 (from google-adk>=1.0.0->-r requirements.txt (line 13))\n",
      "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (2.40.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (2.25.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (5.29.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.26.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0.0,>=2.18.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.6.1)\n",
      "Requirement already satisfied: aiohttp>=3.10 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm->-r requirements.txt (line 14)) (3.12.15)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm->-r requirements.txt (line 14)) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm->-r requirements.txt (line 14)) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm->-r requirements.txt (line 14)) (4.25.1)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm->-r requirements.txt (line 14)) (0.21.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm->-r requirements.txt (line 14)) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r requirements.txt (line 14)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r requirements.txt (line 14)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r requirements.txt (line 14)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r requirements.txt (line 14)) (0.27.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 16)) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 16)) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 16)) (1.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 16)) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 16)) (3.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 16)) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm->-r requirements.txt (line 14)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm->-r requirements.txt (line 14)) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm->-r requirements.txt (line 14)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm->-r requirements.txt (line 14)) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm->-r requirements.txt (line 14)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm->-r requirements.txt (line 14)) (1.20.1)\n",
      "Requirement already satisfied: cryptography in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from authlib>=1.5.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (43.0.3)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-api-python-client>=2.157.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-api-python-client>=2.157.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-api-python-client>=2.157.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=2.157.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (3.2.3)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-aiplatform>=1.95.1->google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (3.36.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-aiplatform>=1.95.1->google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.14.2)\n",
      "Requirement already satisfied: shapely<3.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-aiplatform>=1.95.1->google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (2.1.1)\n",
      "Requirement already satisfied: docstring_parser<1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-aiplatform>=1.95.1->google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.17.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.95.1->google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.95.1->google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.71.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform>=1.95.1->google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.14.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 11)) (1.17.0)\n",
      "Requirement already satisfied: cloudpickle<4.0,>=3.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (3.1.1)\n",
      "Requirement already satisfied: google-cloud-trace<2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.16.2)\n",
      "Requirement already satisfied: google-cloud-logging<4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (3.12.1)\n",
      "Requirement already satisfied: google-cloud-appengine-logging<2.0.0,>=0.1.3 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-logging<4->google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.6.2)\n",
      "Requirement already satisfied: google-cloud-audit-log<1.0.0,>=0.3.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from google-cloud-logging<4->google-cloud-aiplatform[agent-engines]>=1.95.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.3.2)\n",
      "Requirement already satisfied: opentelemetry-resourcedetector-gcp==1.*,>=1.5.0dev0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from opentelemetry-exporter-gcp-trace>=1.9.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.9.0a0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm->-r requirements.txt (line 14)) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.31.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.57b0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp>=1.8.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.4.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp>=1.8.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp>=1.8.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (0.0.18)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp>=1.8.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp>=1.8.0->google-adk>=1.0.0->-r requirements.txt (line 13)) (2.3.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 16)) (3.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 16)) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 16)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 16)) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from cryptography->authlib>=1.5.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography->authlib>=1.5.1->google-adk>=1.0.0->-r requirements.txt (line 13)) (2.22)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 16)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 16)) (3.6.0)\n",
      "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Installing collected packages: websockets, uvicorn\n",
      "\n",
      "  Attempting uninstall: websockets\n",
      "\n",
      "    Found existing installation: websockets 13.1\n",
      "\n",
      "    Uninstalling websockets-13.1:\n",
      "\n",
      "      Successfully uninstalled websockets-13.1\n",
      "\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "  Attempting uninstall: uvicorn\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "    Found existing installation: uvicorn 0.29.0\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "    Uninstalling uvicorn-0.29.0:\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "      Successfully uninstalled uvicorn-0.29.0\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "   -------------------- ------------------- 1/2 [uvicorn]\n",
      "   -------------------- ------------------- 1/2 [uvicorn]\n",
      "   -------------------- ------------------- 1/2 [uvicorn]\n",
      "   -------------------- ------------------- 1/2 [uvicorn]\n",
      "   ---------------------------------------- 2/2 [uvicorn]\n",
      "\n",
      "Successfully installed uvicorn-0.35.0 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ce056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm[proxy] in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (1.75.3)\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.8.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (2.10.1)\n",
      "Requirement already satisfied: aiohttp>=3.10 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (3.12.15)\n",
      "Requirement already satisfied: apscheduler<4.0.0,>=3.10.4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (3.11.0)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (1.24.0)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.25.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (12.26.0)\n",
      "Requirement already satisfied: backoff in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (2.2.1)\n",
      "Requirement already satisfied: boto3==1.34.34 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (1.34.34)\n",
      "Requirement already satisfied: click in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (8.1.8)\n",
      "Requirement already satisfied: cryptography<44.0.0,>=43.0.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (43.0.3)\n",
      "Requirement already satisfied: fastapi<0.116.0,>=0.115.5 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (0.115.12)\n",
      "Requirement already satisfied: fastapi-sso<0.17.0,>=0.16.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (0.16.0)\n",
      "Requirement already satisfied: gunicorn<24.0.0,>=23.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (23.0.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (4.25.1)\n",
      "Requirement already satisfied: litellm-enterprise==0.1.19 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (0.1.19)\n",
      "Requirement already satisfied: litellm-proxy-extras==0.2.16 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (0.2.16)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.10.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (1.12.4)\n",
      "Requirement already satisfied: openai>=1.68.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (1.102.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.7 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (3.11.2)\n",
      "Requirement already satisfied: polars<2.0.0,>=1.31.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (1.32.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (2.11.4)\n",
      "Requirement already satisfied: pynacl<2.0.0,>=1.5.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (1.5.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (1.1.1)\n",
      "Requirement already satisfied: python-multipart<0.0.19,>=0.0.18 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (0.0.18)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (6.0.2)\n",
      "Requirement already satisfied: rich==13.7.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (13.7.1)\n",
      "Requirement already satisfied: rq in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (2.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (0.9.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from litellm[proxy]) (0.21.4)\n",
      "Collecting uvicorn<0.30.0,>=0.29.0 (from litellm[proxy])\n",
      "  Using cached uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting websockets<14.0.0,>=13.1.0 (from litellm[proxy])\n",
      "  Using cached websockets-13.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.34 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from boto3==1.34.34->litellm[proxy]) (1.34.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from boto3==1.34.34->litellm[proxy]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from boto3==1.34.34->litellm[proxy]) (0.10.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from rich==13.7.1->litellm[proxy]) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from rich==13.7.1->litellm[proxy]) (2.19.2)\n",
      "Requirement already satisfied: tzlocal>=3.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from apscheduler<4.0.0,>=3.10.4->litellm[proxy]) (5.3.1)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (1.35.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (1.33.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (4.14.1)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from azure-storage-blob<13.0.0,>=12.25.1->litellm[proxy]) (0.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from botocore<1.35.0,>=1.34.34->boto3==1.34.34->litellm[proxy]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from botocore<1.35.0,>=1.34.34->boto3==1.34.34->litellm[proxy]) (2.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from cryptography<44.0.0,>=43.0.1->litellm[proxy]) (1.17.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from fastapi<0.116.0,>=0.115.5->litellm[proxy]) (0.46.2)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from fastapi-sso<0.17.0,>=0.16.0->litellm[proxy]) (3.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from gunicorn<24.0.0,>=23.0.0->litellm[proxy]) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm[proxy]) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm[proxy]) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm[proxy]) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm[proxy]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm[proxy]) (0.27.0)\n",
      "Requirement already satisfied: anyio>=4.5 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp<2.0.0,>=1.10.0->litellm[proxy]) (4.10.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp<2.0.0,>=1.10.0->litellm[proxy]) (0.4.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp<2.0.0,>=1.10.0->litellm[proxy]) (2.10.1)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp<2.0.0,>=1.10.0->litellm[proxy]) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from mcp<2.0.0,>=1.10.0->litellm[proxy]) (2.3.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm[proxy]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm[proxy]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm[proxy]) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.34->boto3==1.34.34->litellm[proxy]) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from anyio>=4.5->mcp<2.0.0,>=1.10.0->litellm[proxy]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from anyio>=4.5->mcp<2.0.0,>=1.10.0->litellm[proxy]) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from uvicorn<0.30.0,>=0.29.0->litellm[proxy]) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (1.20.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from azure-core>=1.31.0->azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (2.32.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography<44.0.0,>=43.0.1->litellm[proxy]) (2.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from click->litellm[proxy]) (0.4.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm[proxy]) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm[proxy]) (1.0.9)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm[proxy]) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich==13.7.1->litellm[proxy]) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (3.4.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from openai>=1.68.2->litellm[proxy]) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from openai>=1.68.2->litellm[proxy]) (0.10.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from openai>=1.68.2->litellm[proxy]) (4.67.1)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from pydantic[email]>=1.8.0->fastapi-sso<0.17.0,>=0.16.0->litellm[proxy]) (2.2.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from email-validator>=2.0.0->pydantic[email]>=1.8.0->fastapi-sso<0.17.0,>=0.16.0->litellm[proxy]) (2.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm[proxy]) (2025.7.34)\n",
      "Requirement already satisfied: tzdata in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from tzlocal>=3.0->apscheduler<4.0.0,>=3.10.4->litellm[proxy]) (2025.2)\n",
      "Requirement already satisfied: croniter in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from rq->litellm[proxy]) (6.0.0)\n",
      "Requirement already satisfied: redis!=6,>=3.5 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from rq->litellm[proxy]) (6.4.0)\n",
      "Requirement already satisfied: pytz>2021.1 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from croniter->rq->litellm[proxy]) (2025.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from tokenizers->litellm[proxy]) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm[proxy]) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shant\\onedrive\\desktop\\graphragresume\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm[proxy]) (2024.12.0)\n",
      "Using cached uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "Using cached websockets-13.1-cp312-cp312-win_amd64.whl (159 kB)\n",
      "Installing collected packages: websockets, uvicorn\n",
      "\n",
      "  Attempting uninstall: websockets\n",
      "\n",
      "    Found existing installation: websockets 15.0.1\n",
      "\n",
      "    Uninstalling websockets-15.0.1:\n",
      "\n",
      "      Successfully uninstalled websockets-15.0.1\n",
      "\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "  Attempting uninstall: uvicorn\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "    Found existing installation: uvicorn 0.35.0\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "    Uninstalling uvicorn-0.35.0:\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "      Successfully uninstalled uvicorn-0.35.0\n",
      "   ---------------------------------------- 0/2 [websockets]\n",
      "   -------------------- ------------------- 1/2 [uvicorn]\n",
      "   -------------------- ------------------- 1/2 [uvicorn]\n",
      "   -------------------- ------------------- 1/2 [uvicorn]\n",
      "   ---------------------------------------- 2/2 [uvicorn]\n",
      "\n",
      "Successfully installed uvicorn-0.29.0 websockets-13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.10.0 requires uvicorn>=0.34.0, but you have uvicorn 0.29.0 which is incompatible.\n",
      "google-adk 1.10.0 requires websockets>=15.0.1, but you have websockets 13.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install litellm[proxy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0314604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import numpy as np\n",
    "import typer\n",
    "import json_repair\n",
    "import openai\n",
    "import authlib\n",
    "\n",
    "# Project-specific libraries\n",
    "import pypdf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import neo4j\n",
    "\n",
    "import langchain\n",
    "import langchain_openai\n",
    "import langchain_huggingface\n",
    "import langchain_community\n",
    "import langchain_neo4j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e21ad231d98631",
   "metadata": {},
   "source": [
    "# Basic Vector Search and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:45:16.802783Z",
     "start_time": "2025-08-11T12:45:16.798280Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:198: UserWarning: Field name \"config_type\" in \"SequentialAgent\" shadows an attribute in parent \"BaseAgent\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#env setup\n",
    "import os\n",
    "\n",
    "from AgentRunner import AgentRunner\n",
    "\n",
    "#get env setup\n",
    "NEO4J_URI=\"neo4j+ssc://efea2c90.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"7V18VY7NXa1QQl06JD7_FONhdeqSap_7pUMBTgg-o3A\"\n",
    "NEO4J_DATABASE=\"neo4j\"\n",
    "AURA_INSTANCEID=\"efea2c90\"\n",
    "AURA_INSTANCENAME=\"Instance01\"\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    " \n",
    "AUTH = (NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "with GraphDatabase.driver(NEO4J_URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d98fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from functools import wraps\n",
    "\n",
    "# Initialize tokenizer (adjust model name if needed)\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# --- Patch embedding calls to count tokens on input text chunks ---\n",
    "def patch_embedder_instance(embedder):\n",
    "    # Patch embed_documents if exists\n",
    "    if hasattr(embedder, \"embed_documents\"):\n",
    "        original = embedder.embed_documents\n",
    "        def new_embed_documents(texts, *args, **kwargs):\n",
    "            for i, t in enumerate(texts):\n",
    "                tokens = count_tokens(t)\n",
    "                print(f\"[EMBED] Document {i+1}/{len(texts)} input tokens: {tokens}\")\n",
    "            return original(texts, *args, **kwargs)\n",
    "        embedder.embed_documents = new_embed_documents\n",
    "\n",
    "    # Patch embed_query if exists (sometimes separate)\n",
    "    if hasattr(embedder, \"embed_query\"):\n",
    "        original = embedder.embed_query\n",
    "        def new_embed_query(text, *args, **kwargs):\n",
    "            tokens = count_tokens(text)\n",
    "            print(f\"[EMBED] Query input tokens: {tokens}\")\n",
    "            return original(text, *args, **kwargs)\n",
    "        embedder.embed_query = new_embed_query\n",
    "\n",
    "# --- Wrap function for inserting entities into Neo4j to count tokens ---\n",
    "def insert_entities_with_token_count(driver, chunks):\n",
    "    for chunk_idx, records_chunk in enumerate(chunks, start=1):\n",
    "        # Count tokens in 'text' and 'id' serialized (or choose other properties)\n",
    "        total_chunk_tokens = 0\n",
    "        for rec_idx, record in enumerate(records_chunk, start=1):\n",
    "            text = record.get(\"text\", \"\")\n",
    "            rec_id = record.get(\"id\", \"\")\n",
    "            text_tokens = count_tokens(text)\n",
    "            id_tokens = count_tokens(str(rec_id))\n",
    "            total = text_tokens + id_tokens\n",
    "            total_chunk_tokens += total\n",
    "            print(f\"[NEO4J LOAD] Chunk {chunk_idx} Record {rec_idx}: text tokens = {text_tokens}, id tokens = {id_tokens}, total record tokens = {total}\")\n",
    "\n",
    "        # Run your Neo4j batch insertion (adjust to your code)\n",
    "        res = driver.execute_query(\n",
    "            \"\"\"\n",
    "            UNWIND $records AS rec\n",
    "            MERGE (n:Person {id: rec.id})\n",
    "            SET n.text = rec.text\n",
    "            WITH n, rec\n",
    "            CALL db.create.setNodeVectorProperty(n, 'embedding', rec.embedding)\n",
    "            RETURN count(rec) AS records_upserted\n",
    "            \"\"\",\n",
    "            routing_=RoutingControl.WRITE,\n",
    "            records=records_chunk,\n",
    "            result_transformer_=lambda r: r.data()\n",
    "        )\n",
    "        print(f\"[NEO4J LOAD] Chunk {chunk_idx} inserted {res[0]['records_upserted']} records with total approx {total_chunk_tokens} tokens indexed.\\n\")\n",
    "\n",
    "# --- Optional: Print tokens on Neo4j vector index creation ---\n",
    "def create_vector_index_with_token_logging(driver, dimension):\n",
    "    print(f\"[NEO4J] Creating vector index 'text_embeddings' for dimension={dimension} ...\")\n",
    "    driver.execute_query('''\n",
    "    CREATE VECTOR INDEX text_embeddings IF NOT EXISTS FOR (n:Person) ON (n.embedding)\n",
    "    OPTIONS {indexConfig: {\n",
    "        `vector.dimensions`: toInteger($dimension),\n",
    "        `vector.similarity_function`: 'cosine'\n",
    "    }}\n",
    "    ''', dimension=dimension)\n",
    "    print(f\"[NEO4J] Vector index creation triggered for dimension: {dimension}\")\n",
    "\n",
    "# ----- Usage example -----\n",
    "\n",
    "# Patch your embedder instance before use:\n",
    "#patch_embedder_instance(embedder)\n",
    "\n",
    "# When you batch embed, tokens will be printed automatically for each doc chunk.\n",
    "\n",
    "# When inserting batches into Neo4j, use the wrapped insert:\n",
    "#insert_entities_with_token_count(driver, chunks(resumes_with_embeddings))\n",
    "\n",
    "# Create vector index with print:\n",
    "#create_vector_index_with_token_logging(driver, dimension=len(resumes_with_embeddings[0][\"embedding\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2b35fa03837c2",
   "metadata": {},
   "source": [
    "## Load Documents into Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f360aba478ca3c03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:45:18.037934Z",
     "start_time": "2025-08-11T12:45:17.855033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Abhishek_Nandgadkar.pdf (2901 characters)\n",
      "Processed: ABHISHEK_PAWAR.pdf (5924 characters)\n",
      "Processed: Adil_Inthiyaz_Shaik.pdf (3160 characters)\n",
      "Processed: Aditi_Tyagi.pdf (13207 characters)\n",
      "Processed: Aishwarya Gaba_ DataScientist 1.pdf (4581 characters)\n",
      "Processed: AKSHAY_SUBRAMANYA_AITHAL.pdf (6869 characters)\n",
      "Processed: ALOK_MISHRA.pdf (3632 characters)\n",
      "Processed: Aman_Aslam.pdf (2771 characters)\n",
      "Processed: Amit.pdf (4724 characters)\n",
      "Processed: Ankith_M_G.pdf (2887 characters)\n",
      "Processed: Ankush_Salunke.pdf (14065 characters)\n",
      "Processed: Damodharan_Ramalingam.pdf (15131 characters)\n",
      "Processed: Dashrath_Singh_Rajput.pdf (4793 characters)\n",
      "Processed: Deepak_Singh.pdf (2772 characters)\n",
      "Processed: DEEPAK_SONI.pdf (0 characters)\n",
      "Processed: Dheeraj_B.pdf (3551 characters)\n",
      "Processed: Hemant_Gautam.pdf (9648 characters)\n",
      "Processed: Kshitija_Dhamdhere.pdf (2896 characters)\n",
      "Processed: Kuldeep_yadav.pdf (12246 characters)\n",
      "Processed: Laiba_Shaikh.pdf (4199 characters)\n",
      "Processed: Mamta_Resume 0429.pdf (6895 characters)\n",
      "Processed: Naveen_Pasupuleti.pdf (9538 characters)\n",
      "Processed: Nirav_Gandhi (1).pdf (6075 characters)\n",
      "Processed: Punam_Khinde_CV.pdf (3589 characters)\n",
      "Processed: RahulDeokate_Azure_Data_Engineer.pdf (2727 characters)\n",
      "Processed: RishabResume.pdf (2970 characters)\n",
      "Processed: Sahithi_Komirishetti_Resume.pdf (3704 characters)\n",
      "Processed: SaiJyothi.pdf (6683 characters)\n",
      "Processed: Saloni__Vijay_.pdf (7855 characters)\n",
      "Processed: Samiksha_Kolhe_DS_Resume.pdf (5241 characters)\n",
      "Processed: Saurav__Kumar_.pdf (2493 characters)\n",
      "Processed: Sayan_Mitra.pdf (1303 characters)\n",
      "Processed: Sayan_Roy.pdf (3904 characters)\n",
      "Processed: Srushti_Yadav.pdf (3067 characters)\n",
      "Processed: Tanishq's RESUME.pdf (2576 characters)\n",
      "Processed: TanyaResume24062025 (2).pdf (5132 characters)\n",
      "Processed: Yashwanth_Kumar_K.pdf (3472 characters)\n",
      "Total resumes loaded: 37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'HUc3x2rK',\n",
       " 'text': 'Abhishek Nandgadkar\\nData Engineer | Data Analyst | SQL | Python | Spark | Airﬂow | ADF | Big Data | Real-Time Data Processing | Kafka \\n| Cloud Services (AWS, GCP , Azure) | Building High-Performance ETL Pipelines\\nabhisheknandgadkar41@gmail.com 6360952485 Belgaum, Karnataka\\nabhishek-nandgadkar-4760a429b\\nWORK EXPERIENCE\\nNubax Data Labs\\nData Engineer  02/2023 - Present\\n• Designed scalable ETL pipelines for large datasets, optimizing performance.\\n• Built eﬃcient, reliable data workﬂows using Spark and AWS Glue, leveraging Glue jobs for schema \\nmanagement and batch data processing.\\n• Utilized Databricks for distributed data processing, enabling scalable, high-performance analytics and \\nreducing processing times for complex data transformations.\\n• Conducted data proﬁling and validation to meet data governance and quality standards.\\n• Implemented CI/CD pipelines for data workﬂows, reducing deployment times and improving reliability.\\n• Created and maintained Grafana dashboards for real-time monitoring of key metrics and performance.\\nEDUCATION\\nMaratha Mandal Polytechnic\\nDiploma in Computer Science - 86% 2019  - 2022\\nPROJECTS\\nVTS (Vehicle Tracking System) \\n• Designed and developed scalable ETL pipelines using AWS Glue, leveraging Glue jobs and PySpark scripts \\nfor eﬃcient data extraction, transformation, and loading in the VTS system.\\n• Developed real-time alerting systems for geofence and driver behavior events, reducing response times and \\nimproving operational eﬃciency.\\n• Automated operational report generation, reducing manual effort and improving business insights through \\nscheduled Glue jobs and data partitioning.\\n• Built Grafana dashboards to monitor key performance metrics and visualize system health, supporting \\nproactive decision-making and system optimization.\\n• Leveraged Databricks to accelerate data processing and analytics with Spark, improving the performance \\nof large-scale data transformations and enabling real-time analytics for operational decision-making.\\nOrion Data Platform 06/2024  - Present\\n• Focused on robust data ingestion, transformations, and validations for seamless data ﬂows and reporting.\\n• Created workﬂows to transfer data between sources and destinations, ensuring timely, accurate processing.\\n• Optimized data pipelines for better performance, reducing processing time and resource usage.\\n• Designed dynamic transformations (WHERE, GROUP BY, JOIN) to handle complex data processing needs.\\nSKILLS\\n• Programming & Scripting: Python, SQL\\n• Databases: MySQL, PostgreSQL, MSSQL, Snowﬂake, Redis\\n• Big Data & Frameworks: PySpark, Apache Kafka, Pandas, NumPy\\n• Cloud Platforms: AWS (S3, EC2, Glue, IAM, EMR, CloudWatch), Azure (ADLS, Databricks, Data Factory)\\n• Tools & Technologies:  Airﬂow, Power BI, Grafana, Docker, Git, CI/CD, Black, Pylint\\n• Data Engineering: ETL Pipelines, Dynamic Transformations, Data Quality Validation\\n02/2023 - Present'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read pdfs into text with unique id\n",
    "from person import get_short_id\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "\n",
    "\n",
    "#read pdfs into text with unique id\n",
    "def read_resumes_from_directory(directory=\"C:\\\\Users\\\\shant\\\\OneDrive\\\\Desktop\\\\GraphRAGresume\\\\input\"):\n",
    "  \n",
    "    \"\"\"\n",
    "    Read all PDFs from a directory and return a list of text strings\n",
    "    \"\"\"\n",
    "    resumes = []\n",
    "\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return resumes\n",
    "\n",
    "    # Get all PDF files from the directory\n",
    "    pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{directory}'.\")\n",
    "        return resumes\n",
    "\n",
    "    # Process each PDF file\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(directory, pdf_file)\n",
    "\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            text = \"\"\n",
    "            # IMPORTANT Create a unique id for each person.\n",
    "            # In this case we can do it by file name but may vary by use case\n",
    "            # It is important to think about how things are identified in entity extraction\n",
    "            # so that they get properly resolved in the graph\n",
    "            # Usually you do not want to use names.\n",
    "            person_id = get_short_id(pdf_file)\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "\n",
    "            resumes.append({'id': person_id, 'text': text})\n",
    "            print(f\"Processed: {pdf_file} ({len(text)} characters)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {pdf_file}: {str(e)}\")\n",
    "\n",
    "    print(f\"Total resumes loaded: {len(resumes)}\")\n",
    "    return resumes\n",
    "\n",
    "resumes = read_resumes_from_directory()\n",
    "resumes[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f522ca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing embedding chunks: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "from neo4j_graphrag.llm import AzureOpenAILLM\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "from google.adk.agents import Agent\n",
    "from AgentRunner import AgentRunner  # same as you had\n",
    "from google.genai.types import Part, UserContent\n",
    "#add embeddings\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "#from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# embedding_model = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Lightweight, no API needed\n",
    "embedder = AzureOpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        # This should match your Azure deployment name for the embedding model\n",
    "        api_key=\"BmaiYil8P7o3Dgv0JzIEIA4JYd3AHl7Jh6SzBdjkwXfF4DNxCzC3JQQJ99BGACYeBjFXJ3w3AAABACOGZkhi\",  # Your Azure OpenAI API key\n",
    "        azure_endpoint=\"https://gpt-4o-intern.openai.azure.com/\",\n",
    "        api_version=\"2024-12-01-preview\",\n",
    ")\n",
    "\n",
    "\n",
    "def chunks(xs, n=10):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]\n",
    "\n",
    "def batch_embed_resumes(resumes:List[Dict[str,str]], embedder, chunk_size=10) -> List[Dict[str,str]]:\n",
    "    df = pd.DataFrame(resumes)\n",
    "    embeddings = []\n",
    "    # Use tqdm to show progress during embedding generation\n",
    "    for chunk in tqdm(chunks(df['text'], n=chunk_size), desc=\"Processing embedding chunks\"):\n",
    "        # Generate embeddings for each chunk and extend the embeddings list\n",
    "        embeddings.extend(embedder.embed_documents(chunk))\n",
    "\n",
    "    # combine and output\n",
    "    df['embedding'] = embeddings\n",
    "\n",
    "    #print(\"[Embedding] Process completed successfully.\")\n",
    "    return df.to_dict('records')\n",
    "\n",
    "resumes_with_embeddings = batch_embed_resumes(resumes, embedder)\n",
    "resumes_with_embeddings[0]\n",
    "\n",
    "\n",
    "llm = LiteLlm(\n",
    "    model=\"azure/gpt-4o-11-20\",   # 👈 ADK expects this format\n",
    "    api_base=\"https://raginternaffine.openai.azure.com/\",\n",
    "    api_key=\"9iApMYG4ac931NMjWX6cM0AMKhJKsC7Y6tDOPOSGAPSe7lypOGlZJQQJ99BGACMsfrFXJ3w3AAABACOGrjzw\",\n",
    "    api_version=\"2024-12-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34212421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NODE 1] id tokens = 7, text tokens = 656, total tokens = 663\n",
      "[NODE 2] id tokens = 6, text tokens = 1204, total tokens = 1210\n",
      "[NODE 3] id tokens = 5, text tokens = 754, total tokens = 759\n",
      "[NODE 4] id tokens = 4, text tokens = 2690, total tokens = 2694\n",
      "[NODE 5] id tokens = 7, text tokens = 933, total tokens = 940\n",
      "[NODE 6] id tokens = 6, text tokens = 1862, total tokens = 1868\n",
      "[NODE 7] id tokens = 5, text tokens = 817, total tokens = 822\n",
      "[NODE 8] id tokens = 6, text tokens = 568, total tokens = 574\n",
      "[NODE 9] id tokens = 6, text tokens = 1156, total tokens = 1162\n",
      "[NODE 10] id tokens = 5, text tokens = 612, total tokens = 617\n",
      "[NODE 11] id tokens = 5, text tokens = 4007, total tokens = 4012\n",
      "[NODE 12] id tokens = 7, text tokens = 3010, total tokens = 3017\n",
      "[NODE 13] id tokens = 7, text tokens = 1086, total tokens = 1093\n",
      "[NODE 14] id tokens = 5, text tokens = 565, total tokens = 570\n",
      "[NODE 15] id tokens = 5, text tokens = 0, total tokens = 5\n",
      "[NODE 16] id tokens = 5, text tokens = 606, total tokens = 611\n",
      "[NODE 17] id tokens = 6, text tokens = 2175, total tokens = 2181\n",
      "[NODE 18] id tokens = 6, text tokens = 657, total tokens = 663\n",
      "[NODE 19] id tokens = 5, text tokens = 2804, total tokens = 2809\n",
      "[NODE 20] id tokens = 6, text tokens = 871, total tokens = 877\n",
      "[NODE 21] id tokens = 5, text tokens = 1360, total tokens = 1365\n",
      "[NODE 22] id tokens = 6, text tokens = 1954, total tokens = 1960\n",
      "[NODE 23] id tokens = 6, text tokens = 1259, total tokens = 1265\n",
      "[NODE 24] id tokens = 6, text tokens = 731, total tokens = 737\n",
      "[NODE 25] id tokens = 5, text tokens = 542, total tokens = 547\n",
      "[NODE 26] id tokens = 5, text tokens = 707, total tokens = 712\n",
      "[NODE 27] id tokens = 5, text tokens = 863, total tokens = 868\n",
      "[NODE 28] id tokens = 5, text tokens = 1446, total tokens = 1451\n",
      "[NODE 29] id tokens = 7, text tokens = 1525, total tokens = 1532\n",
      "[NODE 30] id tokens = 6, text tokens = 1584, total tokens = 1590\n",
      "[NODE 31] id tokens = 7, text tokens = 517, total tokens = 524\n",
      "[NODE 32] id tokens = 6, text tokens = 290, total tokens = 296\n",
      "[NODE 33] id tokens = 6, text tokens = 821, total tokens = 827\n",
      "[NODE 34] id tokens = 7, text tokens = 722, total tokens = 729\n",
      "[NODE 35] id tokens = 6, text tokens = 620, total tokens = 626\n",
      "[NODE 36] id tokens = 7, text tokens = 1434, total tokens = 1441\n",
      "[NODE 37] id tokens = 7, text tokens = 664, total tokens = 671\n",
      "[TOTAL] Tokens for all 37 nodes: 44288\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize tokenizer (adjust model name if needed)\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def print_token_counts_for_nodes(records):\n",
    "    \"\"\"\n",
    "    Calculate and print tokens used in each node's properties (like 'id', 'text')\n",
    "    before inserting into Neo4j.\n",
    "    \n",
    "    Args:\n",
    "        records: List of dicts, each dict representing a node/entity with properties.\n",
    "                 Expected keys: at least 'id', 'text' (adjust if different)\n",
    "    \"\"\"\n",
    "    total_tokens_all_nodes = 0\n",
    "    for idx, record in enumerate(records, start=1):\n",
    "        id_str = str(record.get(\"id\", \"\"))\n",
    "        text_str = record.get(\"text\", \"\")\n",
    "        id_tokens = count_tokens(id_str)\n",
    "        text_tokens = count_tokens(text_str)\n",
    "        total_node_tokens = id_tokens + text_tokens\n",
    "        total_tokens_all_nodes += total_node_tokens\n",
    "        print(f\"[NODE {idx}] id tokens = {id_tokens}, text tokens = {text_tokens}, total tokens = {total_node_tokens}\")\n",
    "    print(f\"[TOTAL] Tokens for all {len(records)} nodes: {total_tokens_all_nodes}\")\n",
    "\n",
    "# Example usage:\n",
    "print_token_counts_for_nodes(resumes_with_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50091933112bad20",
   "metadata": {},
   "source": [
    "\n",
    "### Result\n",
    "![](img/graph-sample-just-docs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87cf463963380cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:46:29.639370Z",
     "start_time": "2025-08-11T12:46:27.569557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EagerResult(records=[<Record count(n)=37>], summary=<neo4j._work.summary.ResultSummary object at 0x0000020501705BE0>, keys=['count(n)'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# load into People nodes in Neo4j\n",
    "\n",
    "#instantiate driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "#test neo4j connection\n",
    "driver.execute_query(\"MATCH(n) RETURN count(n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "652263471c382d8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:46:31.065859Z",
     "start_time": "2025-08-11T12:46:30.880686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEO4J LOAD] Chunk 1 Record 1: text tokens = 656, id tokens = 7, total record tokens = 663\n",
      "[NEO4J LOAD] Chunk 1 Record 2: text tokens = 1204, id tokens = 6, total record tokens = 1210\n",
      "[NEO4J LOAD] Chunk 1 Record 3: text tokens = 754, id tokens = 5, total record tokens = 759\n",
      "[NEO4J LOAD] Chunk 1 Record 4: text tokens = 2690, id tokens = 4, total record tokens = 2694\n",
      "[NEO4J LOAD] Chunk 1 Record 5: text tokens = 933, id tokens = 7, total record tokens = 940\n",
      "[NEO4J LOAD] Chunk 1 Record 6: text tokens = 1862, id tokens = 6, total record tokens = 1868\n",
      "[NEO4J LOAD] Chunk 1 Record 7: text tokens = 817, id tokens = 5, total record tokens = 822\n",
      "[NEO4J LOAD] Chunk 1 Record 8: text tokens = 568, id tokens = 6, total record tokens = 574\n",
      "[NEO4J LOAD] Chunk 1 Record 9: text tokens = 1156, id tokens = 6, total record tokens = 1162\n",
      "[NEO4J LOAD] Chunk 1 Record 10: text tokens = 612, id tokens = 5, total record tokens = 617\n",
      "[NEO4J LOAD] Chunk 1 inserted 10 records with total approx 11309 tokens indexed.\n",
      "\n",
      "[NEO4J LOAD] Chunk 2 Record 1: text tokens = 4007, id tokens = 5, total record tokens = 4012\n",
      "[NEO4J LOAD] Chunk 2 Record 2: text tokens = 3010, id tokens = 7, total record tokens = 3017\n",
      "[NEO4J LOAD] Chunk 2 Record 3: text tokens = 1086, id tokens = 7, total record tokens = 1093\n",
      "[NEO4J LOAD] Chunk 2 Record 4: text tokens = 565, id tokens = 5, total record tokens = 570\n",
      "[NEO4J LOAD] Chunk 2 Record 5: text tokens = 0, id tokens = 5, total record tokens = 5\n",
      "[NEO4J LOAD] Chunk 2 Record 6: text tokens = 606, id tokens = 5, total record tokens = 611\n",
      "[NEO4J LOAD] Chunk 2 Record 7: text tokens = 2175, id tokens = 6, total record tokens = 2181\n",
      "[NEO4J LOAD] Chunk 2 Record 8: text tokens = 657, id tokens = 6, total record tokens = 663\n",
      "[NEO4J LOAD] Chunk 2 Record 9: text tokens = 2804, id tokens = 5, total record tokens = 2809\n",
      "[NEO4J LOAD] Chunk 2 Record 10: text tokens = 871, id tokens = 6, total record tokens = 877\n",
      "[NEO4J LOAD] Chunk 2 inserted 10 records with total approx 15838 tokens indexed.\n",
      "\n",
      "[NEO4J LOAD] Chunk 3 Record 1: text tokens = 1360, id tokens = 5, total record tokens = 1365\n",
      "[NEO4J LOAD] Chunk 3 Record 2: text tokens = 1954, id tokens = 6, total record tokens = 1960\n",
      "[NEO4J LOAD] Chunk 3 Record 3: text tokens = 1259, id tokens = 6, total record tokens = 1265\n",
      "[NEO4J LOAD] Chunk 3 Record 4: text tokens = 731, id tokens = 6, total record tokens = 737\n",
      "[NEO4J LOAD] Chunk 3 Record 5: text tokens = 542, id tokens = 5, total record tokens = 547\n",
      "[NEO4J LOAD] Chunk 3 Record 6: text tokens = 707, id tokens = 5, total record tokens = 712\n",
      "[NEO4J LOAD] Chunk 3 Record 7: text tokens = 863, id tokens = 5, total record tokens = 868\n",
      "[NEO4J LOAD] Chunk 3 Record 8: text tokens = 1446, id tokens = 5, total record tokens = 1451\n",
      "[NEO4J LOAD] Chunk 3 Record 9: text tokens = 1525, id tokens = 7, total record tokens = 1532\n",
      "[NEO4J LOAD] Chunk 3 Record 10: text tokens = 1584, id tokens = 6, total record tokens = 1590\n",
      "[NEO4J LOAD] Chunk 3 inserted 10 records with total approx 12027 tokens indexed.\n",
      "\n",
      "[NEO4J LOAD] Chunk 4 Record 1: text tokens = 517, id tokens = 7, total record tokens = 524\n",
      "[NEO4J LOAD] Chunk 4 Record 2: text tokens = 290, id tokens = 6, total record tokens = 296\n",
      "[NEO4J LOAD] Chunk 4 Record 3: text tokens = 821, id tokens = 6, total record tokens = 827\n",
      "[NEO4J LOAD] Chunk 4 Record 4: text tokens = 722, id tokens = 7, total record tokens = 729\n",
      "[NEO4J LOAD] Chunk 4 Record 5: text tokens = 620, id tokens = 6, total record tokens = 626\n",
      "[NEO4J LOAD] Chunk 4 Record 6: text tokens = 1434, id tokens = 7, total record tokens = 1441\n",
      "[NEO4J LOAD] Chunk 4 Record 7: text tokens = 664, id tokens = 7, total record tokens = 671\n",
      "[NEO4J LOAD] Chunk 4 inserted 7 records with total approx 5114 tokens indexed.\n",
      "\n",
      "[NEO4J] Vector index creation triggered for dimension: 1536\n"
     ]
    }
   ],
   "source": [
    "from neo4j import RoutingControl\n",
    "\n",
    "#create uniqueness constraint if not exists\n",
    "driver.execute_query(\n",
    "    'CREATE CONSTRAINT IF NOT EXISTS FOR (n:Person) REQUIRE (n.id) IS NODE KEY',\n",
    "    #database_=DATABASE,\n",
    "    routing_=RoutingControl.WRITE\n",
    ")\n",
    "\n",
    "#load\n",
    "\n",
    "insert_entities_with_token_count(driver, chunks(resumes_with_embeddings))\n",
    "\n",
    "\n",
    "# vector index\n",
    "driver.execute_query('''\n",
    "CREATE VECTOR INDEX text_embeddings IF NOT EXISTS FOR (n:Person) ON (n.embedding)\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: toInteger($dimension),\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}\n",
    "''', dimension=len(resumes_with_embeddings[0][\"embedding\"]))\n",
    "\n",
    "# wait for index to come online\n",
    "driver.execute_query('CALL db.awaitIndex(\"text_embeddings\", 300)')\n",
    "print(f\"[NEO4J] Vector index creation triggered for dimension: {len(resumes_with_embeddings[0]['embedding'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e71674210ed09f",
   "metadata": {},
   "source": [
    "## Agent with Document Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "827e35ccd8fac8c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:46:40.833403Z",
     "start_time": "2025-08-11T12:46:34.400276Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\.venv\\Lib\\site-packages\\langchain_neo4j\\vectorstores\\neo4j_vector.py:769: DeprecationWarning: The default returned 'id' field in the search results will be removed. Please switch to using 'elementId' instead.\n",
      "  read_query, filter_params = get_search_query(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\ntext: '),\n",
       " Document(metadata={}, page_content=\"\\ntext: Mr.Ankush  Salunke.  (Software  Developer)   Present  Address-Pune,   Contact  –  +91  7770076005 /  9370544177   Email  –  ankushsalunke7770@gmail.com   LinkedIn:  https://www.linkedin.com/in/ankushsalunke   \\nObjective   To  work  in  a  firm  with  a  professional  work  driven  environment  where  I  can  utilize  and  apply  my   \\nknowledge,\\n \\nskills\\n \\nwhich\\n \\nwould\\n \\nhelp\\n \\nme\\n \\nto\\n \\ngrow\\n \\ntechnically\\n \\nwhile\\n \\nfulfilling\\n \\norganizational\\n \\ngoals.\\n  \\nWith\\n \\nexcellent\\n \\ncommunication\\n \\nskills.\\n \\nResult\\n \\noriented,\\n \\nself-driven,\\n \\nhighly\\n \\nmotivated,\\n \\nsmart\\n \\nand\\n  \\nhungry\\n \\nto\\n \\nlearn\\n \\nnew\\n \\ntechnologies.\\n \\nEager\\n \\nto\\n \\ntackle\\n \\nnew\\n \\nchallenges\\n \\nand\\n \\ncollaborate\\n \\nwith\\n \\nteams\\n \\nto\\n  \\nachieve\\n \\nshared\\n \\ngoals\\n \\nwhile\\n \\nadvancing\\n \\norganizational\\n \\nsuccess.\\n  \\n \\nProfessional  Summary   \\n•  5.10  +  Total  years  of  experience  in  corporate  environment  and   •  3.9+  years’  experience  in  Software  Developer  as  a Python  Django  Developer.  •  UI  development  \\nusing\\n \\nfront-end\\n \\ntechnologieslike\\n \\nHTML,\\n \\nBootstrap,\\n \\nCSS,JavaScript\\n,\\n \\nReact-JS\\n.\\n \\n•\\n \\n2.5\\n \\n+\\n \\nyears’\\n \\nexperience\\n \\nin\\n \\nSoftware\\n \\nDeveloper\\n \\nas\\n \\na\\n \\nNode\\n \\njs\\n \\nExpress\\n \\nframework\\n \\nand\\n \\nfront-end\\n  \\ntechnologies\\n \\nlike\\n \\nHTML,\\n \\nBootstrap,\\n \\nCSS,\\n \\nJavaScript,\\n \\nsound\\n \\nknowledge\\n \\nof\\n  \\nAngular\\n.\\n  \\n •  Handle  data  using  database  like  MySQL,  PostgreSQL,  SQL-server,  MongoDB  \\n•\\n \\nExperience\\n \\nof\\n \\nsoftware\\n \\ndevelopment\\n \\nlife\\n \\ncycle\\n \\n(SDLC)\\n \\n •  Admirable  understanding  of  OOP’s  concept.   •  Good  exposure  with  software  development  and  coding.   •  Experience  in  attending  a  client  meeting  to  discuss  project  related  \\ninformation.\\n \\n•\\n \\nWorking\\n \\nExperience\\n \\nPython\\n \\nLibraries\\n \\nPandas,\\n \\nMatplotlib\\n,\\n \\nNumPy.\\n \\n •  WorkingExperience  on  Version  control  tool  in  Git  and  GitHub.   •  Constituted  restAPIservices  for  applications.   \\nTechnical  Skill   \\n•  Environments  :  Windows  8/10,  Ubuntu   \\n•  Front-End  Technologies  :  HTML5,  CSS3,  Bootstrap  4,  JavaScript,  React.js,AngularJS,  TypeScript.  •  \\nLanguages\\n \\n:\\n \\nPython,\\n \\nC,\\n \\nC++,JavaScript\\n \\n •  Database  :  MySQL,  PostgreSQL,  SQLServer,  MongoDB   •  Framework  :  Django,  DRF  (Django  Rest  Framework),  Express.js  •  Cloud  \\nTechnologies\\n \\n:\\n \\nAWS\\n \\n(Amazon\\n \\nWeb\\n \\nServices),\\n \\nPhysical\\n \\nServer\\n \\n •  Domains  :  Hospitality,  E-commerce,  Finance  &  NBFC-Banking  •  IDE  :  Visual  Studio  Code,  Sublime  Text,  PyCharm   \\nEmployment  History   \\n➢ Company  Name :  ElectronicaFinance  Limited  (EFL)  (Sep  2022  -  Present).    Role  :-  Software  Developer    ➢  Project  Name:  LeadPro  (CRM )  (2022  –  Present)  ➢  Technologies:  Python,  Django,  \\nDjango\\n \\nREST\\n \\nFramework\\n \\n ➢ Project  Descriptions\\n:\\n \\nLeadPro  is  a  Customer  Relationship  Management  \\n(CRM)\\n  \\napplication\\n \\ndesigned\\n \\nto\\n \\nstreamline\\n \\nand\\n \\nautomate\\n \\nthe\\n \\nlead\\n \\nmanagement\\n \\nand\\n \\nsales\\n  \\nprocess\\n \\nfor\\n \\nboth\\n \\ncustomers\\n \\nand\\n \\nemployees.\\n \\nThe\\n \\napplication\\n \\nis\\n \\ndivided\\n \\ninto\\n \\nthree\\n  \\ncore\\n \\nmodules:\\n  \\n ➢ Sales  Module :  Sales  personnel  can  manage  and  track  customer  leads,  schedule   \\nvisits,\\n \\nand\\n \\nconvert\\n \\nleads\\n \\ninto\\n \\ncustomers.\\n \\nThey\\n \\nalso\\n \\nhave\\n \\nthe\\n \\nability\\n \\nto\\n \\nadd\\n \\nnew\\n  \\nleads\\n \\nand\\n \\nupdate\\n \\ncustomer\\n \\ninformation\\n \\nduring\\n \\nthe\\n \\nsales\\n \\nprocess.\\n  \\n ➢ Telecaller  Module :  Telecallers  update  customer  enquiries,  add  information,  and   \\nassign\\n \\nleads\\n \\nto\\n \\nsales\\n \\nrepresentatives.\\n \\nThey\\n \\nplay\\n \\na\\n \\ncrucial\\n \\nrole\\n \\nin\\n \\nthe\\n \\ninitial\\n \\nstages\\n  \\nof\\n \\nlead\\n \\nqualification\\n \\nand\\n \\nensure\\n \\nthat\\n \\nthe\\n \\nlead\\n \\ndata\\n \\nis\\n \\nupdated\\n \\nand\\n \\naccurate.\\n  \\n ➢ Channel  Partner  Module :  Channel  partners  are  able  to  add  leads  as  enquiries  \\ninto\\n  \\nthe\\n \\nsystem,\\n \\ninitiating\\n \\nthe\\n \\nlead\\n \\nmanagement\\n \\nprocess.\\n \\nThese\\n \\nleads\\n \\nare\\n \\nthen\\n \\npassed\\n \\n on  to  the  tele  caller  and  sales  team  for  further  follow-up  and  conversion.   \\n➢  Role  &  Responsibilities:   •  Worked  on  the  backend  development  of  the  LeadPro  CRM  application  using  Django  \\nand\\n \\nPython\\n,\\n \\nensuring\\n \\nefficient\\n \\ndata\\n \\nprocessing\\n \\nand\\n \\nbusiness\\n \\nlogic\\n \\nimplementation.\\n \\n •  Managed  and  maintained  the  PostgreSQL  database,  optimizing  queries,  ensuring  data  \\nintegrity,\\n  \\nand\\n \\nhandling\\n \\ndatabase\\n \\nmigrations.\\n  \\n •  Debugged  and  troubleshot  programming  issues,  ensuring  smooth  application  performance  \\nand\\n  \\nresolving\\n \\nbugs\\n \\nin\\n \\na\\n \\ntimely\\n \\nmanner.\\n  \\n •  Developed  and  exposed  RESTful  web  services  to  facilitate  communication  between  the   \\nfrontend\\n \\nand\\n \\nbackend,\\n \\nenabling\\n \\nseamless\\n \\nintegration\\n \\nwith\\n \\nother\\n \\nservices.\\n  \\n •  Collaborated  closely  with  the  business  team  to  understand  and  implement  business  requirements   \\nas\\n \\noutlined\\n \\nin\\n \\nthe\\n \\nBusiness\\n \\nRequirements\\n \\nDocument\\n \\n(BRD)\\n,\\n \\ntranslating\\n \\nthem\\n \\ninto\\n \\nfunctional\\n  \\nfeatures\\n \\nin\\n \\nthe\\n \\napplication.\\n  \\n •  Participated  in  the  full  software  development  lifecycle,  including  requirements  gathering,  design,   \\ndevelopment,\\n \\ntesting,\\n \\nand\\n \\ndeployment.\\n  \\n \\n➢  Project  Name:  APIHUB  (Finance  Domain)  (Oct  2022  -  DEC  2023)  ➢  Role:  Backend  \\nDeveloper.\\n \\n ➢  Technologies:  Node  JS,JavaScript,  ExpressJS(Backend)   ➢  Responsibilities:   •  Worked  as  a  team  member,  collaborating  with  colleagues  to  achieve  project  goals.  •  Develops  APIs  \\n-\\n \\nCreated\\n \\ncustomAPIs\\n \\nto\\n \\nexpose\\n \\ninternal\\n \\nservices,\\n \\ndata,\\n \\nand\\n \\nbusiness\\n \\nlogic.\\n \\n•\\n \\nIntegrated\\n \\n3rd-Party\\n \\nAPIs:\\n \\nIntegrated\\n \\nexternal\\n \\nAPIs\\n \\nfrom\\n \\nthird-party\\n \\nproviders\\n \\nsuch\\n \\nas\\n \\nAirtel,\\n  \\n Bitly,  CRIF,  Digio-Nach,  EPFO,  GST,  Karza,  SBI,  Aadhar,  PAN,  Surpass,  Digi  locker,  and   \\nvarious\\n \\nemail\\n \\nservices\\n \\ninto\\n \\nour\\n \\ncompany’s\\n \\necosystem,\\n \\nenabling\\n \\ninteroperability\\n \\nand\\n \\nseamless\\n \\ndata\\n  \\nexchange\\n \\nbetween\\n \\nsystems.\\n  \\n •  Shared  APIs:  Developed  microservices  that  expose  APIs  within  our  domain,  allowing  other  teams   \\nor\\n \\nvendors\\n \\nto\\n \\nuse\\n \\nthese\\n \\nAPIs\\n \\nin\\n \\ntheir\\n \\nown\\n \\napplications,\\n \\nservices,\\n \\nor\\n \\nprocesses.\\n  \\n •  Version  Control :  Worked  extensively  with  the  version  control  system  Git ,  ensuring  smooth   \\ncollaboration,\\n \\ncode\\n \\nmanagement,\\n \\nand\\n \\ndeployment.\\n  \\n \\n➢ Project  Descriptions\\n:\\n \\nCentralizedAPI  Management  and  Integration  Platform  \\n➢\\nEnabled\\n \\nseamless\\n \\ndata\\n \\nexchange\\n \\nand\\n \\ninteroperability\\n \\nbetween\\n \\nvarious\\n \\ninternal\\n \\nsystems\\n  \\nand\\n \\nthird-party\\n \\nservices.\\n  \\n ➢ Reduced  manual  efforts  and  enhanced  the  overall  efficiency  of  business  processes  \\nby\\n  \\nautomating\\n \\nintegrations\\n \\nwith\\n \\ncritical\\n \\nthird-party\\n \\nproviders.\\n  \\n ➢ Empowered  various  teams  and  external  vendors  to  leverage  a  wide  range  of  internal  APIs   \\nto\\n \\nintegrate\\n \\ninto\\n \\ntheir\\n \\nservices,\\n \\napplications,\\n \\nand\\n \\nprocesses,\\n \\nfostering\\n \\nbetter\\n \\ncollaboration\\n  \\nand\\n \\nbusiness\\n \\nscalability.\\n  \\n ➢ Security :  OAuth2,  JWT,API  keys   \\n➢  Project  Name:  CLIK  (Loan  Processing  Application)  (2023-2024)  ➢  Technologies:  \\nNode.js,\\n \\nExpress,\\n \\nSQLServer,\\n \\nFlutter\\n \\n ➢ Project  Descriptions\\n:\\n \\nCLIK  is  a  comprehensive  loan  processing  application   \\ndesigned\\n \\nto\\n \\nstreamline\\n \\nand\\n \\nautomate\\n \\nthe\\n \\nlead\\n \\nmanagement\\n \\nand\\n \\nloan\\n \\napplication\\n  processes  for  both  customers  and  employees.  The  application  supports  multiple   loan  journeys,  including:    •  \\nEEL\\n \\nLoan\\n \\nJourney\\n \\n •  \\nCORE\\n \\nLoan\\n \\nJourney\\n \\n •  \\nResidential\\n \\nSolar\\n \\nLoan\\n \\nJourney\\n \\n \\n➢ Customers  can  easily  apply  for  various  types  of  loans,  such  as  property  loans,   vehicle  loans,  and  machine  loans.  The  system  simplifies  the  end-to-end  loan   process,  from  application  to  approval,  ensuring  a  smooth  experience  for  both   customers  and  internal  teams.   ➢  Role  &  Responsibilities:   •  Backend  Development :  Worked  on  the  backend  development  of  the  EFL  CLIK  application   using  Node.js  and  the  Express  framework ,  focusing  on  efficient  data  processing  and  business   logic  implementation  to  handle  complex  loan  workflows.    •  Database  Management :  Managed  and  maintained  the  SQL  Server  database,  ensuring  data   integrity  and  performance  optimization.  Regularly  performed  database  migrations  and   optimized  queries  for  faster  data  retrieval  and  better  application  performance  \\n.  \\n•\\n \\nAPI\\n \\nDevelopment\\n:\\n \\nDesigned\\n \\nand\\n \\ndeveloped\\n \\nRESTful\\n \\nAPIs\\n \\nto\\n \\nfacilitate\\n \\ncommunication\\n  between  the  frontend  and  backend .  Ensured  smooth  data  exchange  with  other  internal  and   third-party  systems,  enabling  seamless  integration  and  improving  the  overall  user  experience.     •  Debugging  and  Troubleshooting :  Identified,  debugged,  and  resolved  programming-related   issues  to  ensure  the  application  ran  efficiently  and  reliably.  Worked  with  the  team  to  fix  bugs,   optimize  the  codebase,  and  enhance  the  application’s  performance.   \\n•  Collaboration  with  Business  Team :  Collaborated  closely  with  the  business  team  to  understand   the  Business  Requirements  Document  (BRD) .  Translated  business  requirements  into   functional  features,  ensuring  the  application  met  the  client’s  needs  and  business  goals.    \\n➢  Company  Name :Axiomtech  Solutions.(Duration:Jun  2020  –  Sep  2022).   ➢  Project  Name:  thepassbox  (E-commerce  site).  (2020-  2021)  ➢  Role:  Software  Developer.   ➢  Technologies:  Python  Django  (Framework),  HTML5,  CSS3,  Bootstrap4,  JavaScript  ➢  Responsibilities:   •  Working  as  a  team  member   •  Producing  the  restful  web  services.   •  Analyze  the  program  to  identify  the  code  changes  as  per  the  business  requirements  •  Worked  extensively  in  using  the  version  control  system  GIT.   •  Implemented  the  search  functionality  for  internal  applications.   •  Used  python  libraries  to  show  the  Product  data  and  details.   •  Involved  in  knowledge  sharing  activities  with  the  team.   •  Worked  in  backend  side  as  well  as  frontend  technologies.  CreatingRest-APIalso.   •  Develop  platforms  for  displaying  data  visually.  •  Working  in  tandem  with  designers  and  developers  of  software.  •  Create  an  API  backend  and  an  app  architecture  that  can  grow  with  your  business.     •    Developing  features  that  are  visible  to  users  using  the  in-built  React  toolset.  \\n  \\n➢  Project  Descriptions\\n:\\n \\nproject  is  kind  of  e-commerce  but  instead  of  product  sale  we  give   \\nproducts\\n \\non\\n \\nrent.\\n \\nAll\\n \\ncustomers\\n \\ncan\\n \\nbuy\\n \\nproducts\\n \\non\\n \\nrent\\n \\nand\\n \\npay\\n \\n20%\\n \\nupfront.\\n \\nOnce\\n \\na\\n \\nproduct\\n \\nis\\n \\ndelivered\\n \\n,\\n \\nthe\\n \\ncustomer\\n \\nwill\\n \\npay\\n \\na\\n \\npending\\n \\namount.\\n \\nThe\\n \\nrenting\\n \\ncalculation\\n \\non\\n \\nhourly\\n \\nand\\n \\ndaily\\n  \\nbasis.\\n \\n ➢  Project  Name:  OneCupOneCar  (waterless  car  washing )  (  Feb  22-  Dec  22)  ➢  \\nTechnologies:\\n \\nPython,\\n \\nDjango,\\n \\nDjangorestframework,\\n \\n ➢ Project  Descriptions\\n:\\n \\nThis  application  is  useful  for  customer  and  \\nemployee\\n  \\nwashing\\n \\ncar\\n \\nbut\\n \\nwaterless\\n \\nservices\\n \\ncover\\n \\nthe\\n \\nwhole\\n \\nspectrum\\n \\nof\\n \\nvehicle\\n  \\ndetailing,\\n \\nincluding\\n \\nexterior\\n \\n&\\n \\ninterior\\n \\ndetailing,\\n \\npaint\\n \\ncorrection\\n \\n&\\n \\npaint\\n  \\nprotection,\\n \\npre-\\n \\n&\\n \\npost-sale\\n \\ndetailing,\\n \\ncustomer\\n \\ncan\\n \\nbook\\n \\nonline\\n \\nslot,\\n  \\ncustomer\\n \\nstore\\n \\ninformation\\n \\nonline\\n \\npayments\\n \\ngateway,\\n \\nuser\\n \\nlogin\\n \\ndetails,\\n  \\nvehicle\\n \\ndetails\\n \\nnow\\n \\nlaunching\\n \\nsoon\\n \\nmobile\\n \\napp.\\n  \\n \\n➢  Role  &  Responsibilities:   •  \\nWorking\\n \\non\\n \\nthe\\n \\nbackend\\n \\npart\\n \\nof\\n \\nan\\n \\napplication\\n \\nin\\n \\nDjango\\n \\nusing\\n \\npython\\n \\nand\\n \\nmaintaining\\n \\na\\n \\ndatabase.\\n \\n•\\n \\nDebugging\\n \\nand\\n \\ntroubleshooting\\n \\nprogramming\\n \\nrelated\\n \\nissues.\\n \\n •  Producing  the  restful  web  services.   •  Analyze  the  program  to  identify  the  code  changes  as  per  the  client  requirements   •  Getting  thoroughly  involved  in  the  programming  of  web-based  applications.  ➢  Project  Name :  CatchApp  (Hotel  Booking)   ➢  Technologies :  Python,  Django,  DRF,  JavaScript,  React  -JS,   ➢  Responsibilities:   •  Working  as  a  team  member.   •  Responsible  for  UIdesign  and  backend  development  according  to  client  requirement  and  bug  \\nfixing\\n \\n.\\n \\n ➢  Project  Descriptions:  Catch  App  is  a  smart  tool  for  group  travel  managers  to  simplify   \\nthe\\n \\nplanning.\\n \\nBooking\\n \\nand\\n \\nmanaging\\n \\nthe\\n \\ngroup\\n \\ntravels.\\n \\nIt\\n \\nprovides\\n \\na\\n \\nfacility\\n \\nto\\n \\nmonitor\\n  \\ntour\\n \\ndetails.\\n \\nHotel\\n \\nbookings\\n \\ntour\\n \\nmanagement\\n \\nand\\n \\nbid\\n \\nrequest\\n \\nto\\n \\nhotels\\n  \\n \\n➢  Febtech  Industries  (June  -2019  to  june-2020)  •  Role:  Project  Engineer  and  Project  Management   •  Skill:  Tally,  SAP,  Excel  and  MS   \\n•  Responsibilities:   •  Working  as  team  leader.   •  Designing  and  development  engineering  system.   •  Daily  attending  meetings  with  the  team  to  maintain  relationships  with  clients,  \\nvendors.\\n \\n•\\n \\nPlanning\\n \\nas\\n \\nper\\n \\nclient's\\n \\nrequirement.\\n \\n •  Involved  in  knowledge  sharing  activities  with  the  team.   •  Utilized  HTML,  CSS  and  various  design  tools  for  front-end  development  and  design-related   \\ntasks.\\n \\n •  Working  Experience  on  a  teamleader,  project  management,  planning,  and  project   \\ndelivery\\n \\nwithin\\n \\ntime.\\n  \\n \\n➢  Languages                                                        INTERESTS  :   Reading/Travelling/  Trekking   English                                                   Professional  Working  Proficiency   Hindi   Full  Professional  Proficiency    Marathi   Native  or  Bilingual  Professional   \\n \\n Educational  Qualification   \\n   \\n \\nUniversity   Examination   Year  of  passing   \\nPercentage   Class/Remark  \\nPune   B.E.   2019   59.15%   Second  Class  \\nMSBTE   Diploma   2016   68.24%   First  Class    I  hereby  declare  that  above-mentioned  information  is  correct  to  the  best  of  my  knowledge  and   \\nbelief.\\n \\n \\nPlace:  Pune  Yours  Sincerely  \\n Date:  AnkushSalunke   \"),\n",
       " Document(metadata={}, page_content='\\ntext:  Tanya  Mishra\\n                                      \\n Tanya  Mishra  \\nBIT\\n \\nMesra-Ranchi\\n                       \\n \\nmishratanya1008@gmail.com     (+91)70009393543   Awadhpuri,Bhopal  M.P                     \\nWork  Experience  \\n \\nML  Intern(Airex  Lab)  IISC  Bangalore  India  Dec  2024-Jan  2025  •  Skills :Deep  learning,  JAX,  EQUINOX,  Real  world  data,  CNN  •  Configured  hardware  systems  for  optimized  performance  while  integrating  advanced  AI  frame-  works,  contributing  practical  insights  that  identified  three  critical  challenges  affecting  system  architecture  efficiency  during  implementation  phases  of  new  projects.Comfortable  conveying  complex  AI  concepts  to  \\nnon-technical\\n \\nusers\\n \\nthrough\\n \\ndemos\\n \\nand\\n \\npresentations.\\n •  Developed  algorithms  that  optimized  data  processing  workflows,  reducing  computation  time  by  40%  while  mastering  emerging  frameworks  like  JAX  and  Equinox.Collaborated  with  cross-functional  researchers  and  \\ncontributed\\n \\nto\\n \\nconference-level\\n \\ndocumentation.\\n •  Contributed  to  open-source  projects  like  the  ScireX  library,  and  participated  in  organizing  the  CASML  2024  International  Conference.Troubleshot,issue  resolution  and  optimized  real-world  AI  deployments,  identifying  \\nsystem-level\\n \\nbottlenecks\\n \\nand\\n \\ncontributing\\n \\nto\\n \\nimproved\\n \\nuptime\\n \\nand\\n \\nreliability.incident\\n \\nresponse,technical\\n \\nsupport,customer\\n \\nsuccess,\\n  Education  Ranchi  ,  India  BIT  Mesra  2022  –  2026  •  B.Tech  in  Computer  Science  and  Engineering.CGPA:  8.89.  •  Undergraduate  Coursework :  Data  Structures  and  Algorithms,  Operating  Systems,  Computer  Networks,  Ma-  chine  Learning,  Data  Mining,  Optimization  Techniques,  Software  Development,  Formal  Languages  and  Automata  Theory,  Software  Engineering   Technical  Projects  \\n \\n•  SmartTalk(Smart  Offline  LLM  Assistant  for  Mute  Users): Designed  and  deployed  a  GenAI-powered  offline  voice  \\nassistant\\n \\nfor\\n \\nmute\\n \\nusers\\n \\nby\\n \\nintegrating\\n \\n4+\\n \\nopen-source\\n \\ntools\\n \\n(Whisper,\\n \\nTinyLlama,\\n \\nPiper,\\n \\nSilero),\\n \\nDeployed\\n \\non\\n \\nDocker\\n \\n+\\n \\nKubernetes\\n \\nacross\\n \\nsimulated\\n \\nedge\\n \\nenvironments\\n \\nfor\\n \\n10x\\n \\nscaling,\\n \\nshowcasing\\n \\nhardware-free\\n \\nscaling\\n \\nand\\n \\nstability.Integrated\\n \\nWhisper\\n \\nand\\n \\nTinyLlama\\n \\n(LLM)\\n \\nfor\\n \\nlow-latency\\n \\noffline\\n \\ninference,\\n \\nsupporting\\n \\nuser-centric\\n \\nGen-AI\\n \\ninteractions.Reduced\\n \\nlatency\\n \\nby\\n \\n40%\\n \\nvia\\n \\nmodular\\n \\nNLP\\n \\noptimization.Skill-Python,\\n \\nGenAI,\\n \\nDocker,\\n \\nKubernetes,\\n \\nSpeech\\n \\nProcessing,\\n \\nEdge\\n \\nComputing,\\n \\nAccessibility\\n \\nSystems.\\n •  Multimodal  Medical  Image  Reconstruction(CV)-  Built,  training,  achieving:  Built  a  3D  CVAE  with  4-layer  \\nencoder/decoder\\n \\n(64–256\\n \\nfilters),\\n \\ntraining\\n \\non\\n \\nBraTS2020,\\n \\nachieving\\n \\n85%+\\n \\nDice\\n \\nfor\\n \\n4-class\\n \\ntumor\\n \\nsegmentation\\n \\nvia\\n \\nU-Net\\n \\n(5-level,\\n \\n32–512\\n \\nfilters),\\n \\nleveraging\\n \\nAWS\\n \\nfor\\n \\nscalable\\n \\nevaluation\\n \\nand\\n \\nreal-time\\n \\nvisualization.Skills-Python,,\\n \\nGenAI,\\n \\n3D\\n \\nConvolutional\\n \\nNeural\\n \\nNetworks,\\n \\nVariational\\n \\nAutoencoders\\n \\n(VAE),\\n \\nMedical\\n \\nImage\\n \\nSegmentation,\\n \\nBraTS2020,\\n \\nDice/IoU\\n \\nMetrics,\\n \\nAWS\\n \\nEC2/S3,\\n \\nNumpy,\\n \\nMatplotlib,\\n \\nNibabel,\\n \\nDeep\\n \\nLearning,\\n \\nBiomedical\\n \\nImaging.\\n •  Amazon  box  length  prediction(NLP)- Developed,  processing,  predicting:  Developed  an  NLP  regression  model  using  \\nAmazon\\n \\nproduct\\n \\ndata,\\n \\nprocessing\\n \\n450+\\n \\ncleaned\\n \\nsamples\\n \\n(HTML,\\n \\nURLs,\\n \\nemojis\\n \\nremoved),\\n \\ntokenizing\\n \\nand\\n \\npadding\\n \\nto\\n \\n128-length\\n \\ninputs,\\n \\npredicting\\n \\nbox\\n \\nlengths\\n \\nwith\\n \\n5.2\\n \\nmean\\n \\nand\\n \\n5.1\\n \\nmedian,\\n \\nenabling\\n \\naccurate\\n \\npackaging\\n \\ninsights.Skills-Python,\\n \\nNLP,\\n \\nTensorFlow/Keras\\n \\nor\\n \\nPyTorch,\\n \\nTokenization,\\n \\nText\\n \\nPreprocessing,\\n \\nHTML\\n \\nParsing,\\n \\nRegular\\n \\nExpressions,\\n \\nSequence\\n \\nPadding,\\n \\nRegression\\n \\nModeling,\\n \\nPandas,\\n \\nNumpy,\\n \\nData\\n \\nVisualization,\\n \\nStatistical\\n \\nAnalysis.\\n •  Emulsify  (Emotion  and  Stress-based  Song  Recommendation  System)(ML  DL)- Emulsify  (Emotion  and  \\nStress-based\\n \\nSong\\n \\nRecommendation\\n \\nSystem)-Emotion\\n \\nRecognition:\\n \\nimplemented\\n \\nreal-time\\n \\nemotion\\n \\ndetection\\n \\nusing\\n \\nOpenCV’s\\n \\nCascade-Classifier,\\n \\nattaining\\n \\n90%\\n \\nrecognition\\n \\naccuracy;Song\\n \\nRecommendation:\\n \\nSpotify\\n \\nAPI\\n \\nintegration,\\n \\nensemble\\n \\ntechniques,\\n \\n99.49%\\n \\naccuracy.\\n Additional  Experience  and  Awards  \\n \\n•  Gate  DA  Qualified-(Rank-10928)  Probability  and  Statistics,Linear  Algebra,Calculus  and  Optimization,Programming,  Data  Structures  and  Algorithms,Database  Management  and  Warehousing,Machine  Learning,AI  •  LeetCode(300+)  problem-Solving   Data  Structures  and  ALgorithms,MySQL,Pandas,Databases  System   Languages  and  Technologies  \\n \\n•  Programming  Languages:  C/C++,  Java,  Python,  SQL  •  Technologies  &  Tools:  Machine  Learning,  Software  System,  Software  Design,  Database,  MySQL,  Microsoft  PowerBI,  Microsoft  Excel,  Linux/Unix,  Datasets,  Statistical  Analysis,deep  Learning,Computer  vision,Natural  \\nlanguage  processing.Gen-AI  Frameworks:  TinyLlama,  Whisper,  Silero,  HuggingFace  Transformers.Cloud  Platforms:  AWS   '),\n",
       " Document(metadata={}, page_content='\\ntext: About:\\n• An Enthusiastic Data Scientist with experience in working over variety of Data Science aspects in\\nML & DL, including – Azure Open AI based solutions, improving LLMs architecture, functionating\\nGraphical data bases and using them with LLMs and Gen AI, NLP based solutions, Azure OCR based\\nextractions, Web automation using Beautiful Soup & Selenium & Classical ML Techniques &\\nStatistical Modelling: Time Series. Deploying ML-DL and advanced models.\\n• Recent Projects: LLM using Azure Open AI, Azure OCR, Ensemble Learning, NLP based solution,\\nRAG – LLM, Gen AI.\\n• Along with experience, duly equipped with Post-Graduation in Machine Learning and AI.\\nSkills and Expertise:\\n• Experienced in Developing & Implementing ML, DL, models using – Azure Open AI, Azure OCR,\\nPython, Docker, Flask & PySpark. Clouds: AZURE Open AI, AWS – Bedrock\\n• Recent project: Agentic AI assistive Bot using Azure, Dedicated Bot using Azure Open AI,\\nAzure OCR – extraction of data in Key Value pairs, Image Recognition, Time Series\\nForecasting, ML - XGBoost, NLP solutions – using NLP techniques to improve extraction\\nresults, Improve suggestive similar vectors that could be later used for Tuning purposes,\\nLangChain – GenAI, LLAMA Index, RAG - LLM models for Chat-Bot & automation tasks,\\nFine Tuning. Optimizations: Optuna, Hyperband, TPE Sampler.\\n• Parallel web extraction & web automation works for the support of existing AI solutions using tools\\nlike OCR, BS4, and Selenium. Parallel support to other teams in updating SPs.\\n• Equipped with a variety of soft skills such as: public speaking and decision making.\\nWORK EXPERIENCE: (6 Years)\\nCurrent Designation: • Data Scientist\\nCompany: • ASTERI Tech: TimeForge & CBC- (May: 2024 – Dec: 2024)\\nRoles & \\nResponsibilities:\\n• Collaborating with Client & cross-functional teams to translate business\\nrequirements into data-driven solutions, converting ideas into enterprise-ready\\napplications. Ensure accuracy, timely delivery & analyze deliverables.\\n• Using RAG for LLM models for Dedicated BOTs and Automation tasks.\\nKuldeep Yadav\\nData Scientist\\ny.kuldeepg07ko@gmail.com | LinkedIn | 9413022444, 7878727746\\nCurrent Designation:\\nCompany:\\nRoles & \\nResponsibilities:\\n• Building and maintaining robust end to end solutions using: Azure OpenAI based\\nDedicated Bots using RAG, Agentic AI based dedicated bots, Azure OCR based\\nextractions.\\n• Designing & implementing solutions in support of ongoing projects like: NLP &\\nComputer Vision based solutions to improve OCR results, Neo4j and other\\nGraphical DBs to be involved in layers of Tuned KB, Using Agentic AI to support\\nexisting Dedicated Bots, Automation based Bots using Azure Open AI.\\n• Building end to solutions including developing APIs, scheduling APIs,\\nScheduling Calls on Deployment Servers. Updating Stored Procedures with\\nlatest extractions.\\n• Working with Clients Cross functional teams and managing timely. Dividing\\nteam members for dedicated stuffs. Working on R&D for upcoming projects.\\n• Clouds: AZURE Open AI\\n• Data Scientist\\n• Contract to Hire Role – Anlage Infotech (Dec: 2024 – March: 2025)Designation:\\nCompany:\\nRoles & \\nResponsibilities:\\n• Data Scientist & Developer – Independent Contract\\n• Channel Partner Firm: Jiten R  (2023 Nov – 2024 March)\\nDesignation: • Researcher\\nCompany: • ISTE: MNIT (2019 Feb – 2021 APR)\\nRoles & \\nResponsibilities:\\nDesignation:\\nCompany:\\nRoles & \\nResponsibilities:\\n• Extracting data insides, getting useful info about trends & patterns & generating \\ndeliverables. Collaborating with different Departments & analyze the deliverables.\\n• Build, implement and optimize ML, LLM & AI models & solutions using  XG Boost, \\nARIMA, SARIMA, GEMINI 1.5 and GPT 3\\n• This profile involves projects from 3 Departments: Samsung USA (Sales Team) -\\nML – Classification & Ensemble Techniques based & Statistical modelling -based \\nTime Series Forecasting. Secondly, Samsung Store (Wholesale venture) & Tech \\nSupport – NLP: LLM & AI based bot projects for this website.\\n• Designing and maintaining – Web Frameworks: FLASK API. Integrating CI/CD \\nPipelines – GIT Hub.\\n• SME (Data & Tech. Analysis)\\n• TP: SAMSUNG (Client) (2021 May – 2023 November), Location: Chandigarh\\n• Build, implement and optimize ML & DL models using XG Boost, Fb Prophet,\\nARIMA, SARIMA, LLM tasks, NLP- Sentimental Analysis and other libraries.\\n• Use latest Optimization techniques like: Optuna- Hyperband. Apply sMAPE, RMSE\\nbased metrics & optimization algorithms including Simulated Annealing and\\nConstraint Programming.\\n• Image detection and classifying Images with Text separately. Text Extraction\\nfrom images data (OCR, PyTesseract), improving Image content using Open CV.\\n• Stay connected latest industry trends & mentor/guide team members on them.\\n• Worked on numerous Time-Series projects, from historical data to hourly,\\nforecasting accurately from months to few hours specific events as needed.\\n• Designing and maintaining – Web Frameworks: FLASK API & FAST.\\n• Maintaining CI/CD Pipelines – GIT Hub & get Integrated with Frameworks.\\n• Worked on LLM – Automation based Bot projects using OpenAI.\\n• Clouds: AZURE Open AI, AWS – Bedrock.\\nPROJECTS:\\nProject Title: “Initially it involves Web extraction using Beautiful Soup - 4 &\\nSelenium followed by details extraction using Azure OCR & finally\\nusing extracted details as input for 2nd AI Agent.\\nThis project assures end to end automation of – Applications input process for\\nour client from Insurance domain. That enhances the o/p of our Bot (Agent 2).\\n• The initial phase was extracting and downloading data from website it has\\nmultiple layers even pages with multiple logins and time limit dedication so we\\nhave used BS-4 & Selenium that ensures complete extractions. Followed by\\nAzure OCR to extract required data from downloads in Key-Value pairs. For\\nkey-value pair extraction we have used Proper Layout model.\\n• Later, using Computer Vision and NLP we have improved the accuracy of OCR\\noutput. The API structure was storing the final K-V pair output in a SP.\\nProject \\nOutcomes:\\nSuccessfully implemented and deployed: A) Azure OCR: Proper Layout model \\nwith about 95% accuracy. B) Scheduled the BS-4 task for i/p indications.\\nActions:\\n• Worked on 10+ Research Projects that got published and deployed on Python.\\n• 5G Segmentation Analysis: Analytical Reduction Approach, Project that got \\ndeployed using Statistics & Clustering algorithm: KCN\\n• Worked on some Projects & PSQL based Developing Tasks: with a team of 3. \\nInitially worked on ML & Statistical Projects based on XGBoost,  Ensemble and \\nTime-Series. Later worked on 2 Gen AI projects: User Assistance Bot and \\nAutomation based Bots.Project Title: “PDF – BOT a bot dedicated to special invitees of a special event.\\nDeploying it separately on a dedicated platform for the client: CBC”.\\nA LLM based chat bot tool considering the requirements of the client, a\\ndedicated web hosted link for this automated bot and that must be dedicated\\nonly to the data provided (that was in form of a PDF).\\nActions:\\n• Since the data source was dedicated so we have used the PyPDF Library.\\nConsidering the size of Data, fast retrieval was a major issue. I tried solving it\\nusing LangChain structure a GEN AI based transformer model with FAISS.\\n• The data source was large, so I have chunked the PDF into smaller parts\\n(chunk_size = 1000). To ensure the accuracy and functioning of embeddings I\\nhave used Gemini 1.5 Pro.\\nProject \\nOutcomes:\\nSuccessfully implemented and deployed: A) RAG Pipelined: chat -bot. B) \\nDeployed and web-hosted the bot on a dedicated platform with a Link \\naccessible feature.\\nProject Title: “Time Hour based Sales Prediction”\\nTo forecast the sales of upcoming week or month. Else to get detailed \\npredictions about items that will be in demand for upcoming week or \\nmonth. Helping a client from the food industry.\\nActions:\\n• Considering some special event, the data provided has hourly sales data\\nhaving details about items, their quantity, price and cooking time. So, we\\ncan move towards an – aggregate towards day-shift sales with detail info\\nabout items. Accordingly, the EDA and Visualizations involved hourly sales\\nmapping and with SARIMA we can efficiently forecast next 30 days of\\nsales and also visualize with graphs like: 1 day and 30 day forecast.\\n• Finaly deploying Random Forest and XG Boost models to validate.\\nProject \\nOutcomes:\\nSuccessfully analyzed: A) Most important factors affecting sales. B) Sales \\nforecasts for a day & 30 days. C) Deployed the Model for Client’s ease.\\nProject Title: “Develop and Implement a ML model for successfully classifying\\nConsumer Loan products files as Case-File Category with negative\\nstatus at setup Branch but, are in the criteria range above 40%\\nfor the Channel Partners”\\nHelping the Channel partners in classifying the eligible Case-Files, in\\nsanctioning criteria of above 40%, that’s 15% below Banks criteria of\\nscope. Reducing the Loan Disbursement Time by 25%. Parallelly for\\nBanks,forecasting the defaulters for Consumer Loan products.\\nActions:\\n• Data Processing: Visualizing & EDA.\\n• Balancing the imbalanced variables using – SMOTE. To build a stable and \\nbest optimal classification model for this categorical data.\\n• Created successfully 4 classification models and their accuracies were -\\nDecision Tree model: 70%, optimizedthe hyperparameters and improved \\nperformance through Grid-Search CV: 82%, Random Forest approach \\nevaluated using Cross-validation: 83.6% and finally AdaBoost: 95%. \\nFinally with a report card and proceed with optimal solution.\\nProject \\nOutcomes:\\n• A) Successful model creation that enhances the capability of Bank \\nto forecast the riskier customers by 95%. B) Deployed the Model for\\nthe convenience of our Client. (a Channel Partner Firm, Jaipur)ACHIEVEMENTS:\\n● 2nd Rank – Schneider Electric Innovation Challenge (Go Green – Barcelona event 2019)\\n● RESEARCH Published: 5G Segmentation Analysis – Analytical Reduction Approach (2021)\\n: MECT (2019)\\nReferences: • Harish Kumar\\no Lead Data Scientist | Generative AI | IRIS Software Group\\no +91 9654209388| harishkumar14590@gmail.com\\n• Programming Languages: Python, C, C++\\n• Libraries:Pandas, NumPy, Scikit Learn, TensorFlow, Keras, PyTorch, OpenCV, SciPy, Stats Model\\n• Techniques: Regression, Classification,Ensemble Learning, Random Forest, XG Boost, DL, Time \\nSeries– ARIMA, SARIMA, Pro Phet, Word Embeddings, NLP, LLM, OPEN AI, GEN AI\\n• Tools: Jupyter Notebook, MySQL Workbench, Google Colab, Visual Studio Coding, Streamlit\\n• Good Knowledge of Stats: Normal Distribution, Hypothesis Testing, Z-test and T-test.\\n• MySQL, Advanced EXCEL, Command Line Programming\\n• TABLEAU\\n• FLASK, AWS, AIRFLOW, Containers based approach\\n• NLP, LLM, RAG, GENAI, GEMINI\\n• ANACONDA\\nTECHNICAL SKILLS:\\nACADEMIC PROFILE:\\n2024 Imarticus Learning\\n• Post Graduation – Machine Learning and Artificial Intelligence\\no Gen AI \\no Programming Language - Python\\no Machine Learning and AI\\no SQL\\no Tableau\\nJaipur, India\\n2020 National Testing Agency (NTA)\\n• Council of Scientific and Industrial Research – CSIR (NET): \\nExam\\nJodhpur, India\\n2019 Rajasthan Technical University\\n• B.Tech.\\no Electronics and Communication Engineering\\nJodhpur, India\\nProject Title: “Assistive: AI” : It’s an end-to-end assistive bot using Azure OpenAI GPT-4o\\n– that handles accepting info from newly added Live data (that might be\\ntext, excel sheets or images) & already provided source i/p KB & 15+ SPs.\\nA RAG pipelined end to end LLM based Bot that serves as a dedicated Bot to assist\\nboth the customers and channel branches of our client from Insurance domain.\\n• It takes input from both a dedicated i/p source KB & additional 15+ SPs & newly added Live \\ndata (that might be text, excel sheets or images). Considering the size of input data and \\nunderstanding that its real time value is dynamic based on various factors so, we have \\nmoved from Neo4j to REDIS + FAISS concept  (Neo4j: dropped, cause of high response \\ntime with new added data). \\n• To assure the accuracy with involvement of newly added data we structured a RAG pipelined \\nstructure along with partial Fine-tuning where we added some dedicated references based on \\ncriteria of each input section.\\nActions:\\nProject Outcomes: Successfully implemented and deployed: RAG Pipelined: dedicated bot.'),\n",
       " Document(metadata={}, page_content='\\ntext: DAMODHARAN RAMALINGAM \\n                         Contact No : 8248856586 \\n                         Mail Address : damo52760@gmail.com \\n \\n \\n                                         \\n  \\n \\n                                         \\n \\n• 14.6 years of experience as a Senior Technical Lead in renowned IT organizations. \\n• Involved in requirements gathering, analysis, design, development, testing, and \\ndeployment of web-based applications. \\n• Extensive experience in building web/client applications using .NET Core  and Angular-\\nbased technologies. \\n• Over 14 years of expertise in .NET web application development. \\n• 7+ years of team leadership experience, managing offshore teams. \\n• Skilled in integrating applications with third -party APIs  for seamless data retrieval and \\ndisplay. \\n• Strong problem-solving skills with a track record of resolving complex software issues. \\n• Hands-on experience in application maintenance, enhancements, and support. \\n• Effective communicator and team facilitator with a fast-learning mindset. \\n• Ability to prioritize tasks, work independently, and collaborate within teams  to drive \\nproject success. \\n \\n  \\nTechnical Skills Summary \\n• Primary Role: .NET Applications Lead Developer \\n• Operating Systems: Windows 10 \\n• Programming Languages: Blazor, C#.NET, VB.NET, Angular, SSIS, SQL Server 2015, WPF \\n• Frameworks: .NET 8, .NET Core (3.1, 2.1), .NET Framework \\n• Web Technologies: .NET Core, ASP.NET MVC, ASP.NET \\n• Microservices & Cloud:  .NET Core, Kubernetes with containers, SQL with CI/CD \\nintegration \\n• Development Tools: Visual Studio 2019, SSST, SSMS 2015 \\n• Web Services: Web API, WCF \\n• Third-Party Integrations: Syncfusion Controls, Dealhub, Chargebee, Microsoft Graph API \\n• Scripting & Frontend: Angular 16, JavaScript, jQuery, AJAX \\n• Databases: SQL 2019, SQL 2015 \\n• Version Control: TFS, Azure DevOps \\n• Domain Expertise: Finance, Taxation, Manufacturing \\n• Others: SignalR \\n \\nEXPERIENCE \\n \\nOrganization Designation Duration  \\nAshley GCC Tech Lead 1.6 months \\nGAVS Tech Lead 6 months \\nG2 Technologies Senior Tech Lead 10 months \\nCustomer Analytics Senior Project Lead 4.10 months \\nSuperior Innovative Technologies Team Lead 6 months \\nRamco Systems Senior S/W Devloper 5.6 years \\nPERI Software Technical Cordinator 6 months \\n \\n \\n \\nEDUCATION \\n \\nDegree College / University \\nB Tech St .Joseph’s College of Engineering / Anna \\nUniversity \\n  \\nPROJECT SUMMARY \\n \\n1. ADJUSTMENT SURCHARGE  \\nMicroservices Environment : .net 8, SQL Server 2019 deployed on Kubernetes service  \\nWeb API Environment : ASP.Net Core Web API with SQL Server 2019 \\nConsole Application Environment : C# .Net \\n \\nDescription : \\nDeveloped a scalable, lightweight, and modular microservice architecture using .NET to \\nhandle specific business functionalities independently while ensuring seamless \\ncommunication with other services. The purpose of the surcharge microservice is to \\ncalculate the surcharge accurately based on item numbers, customer data and adjustment \\ndetails which will be helpful for the business to put the order and generate the invoice. \\nFocused extensively on optimizing performance, ensuring efficiency as it serves multiple \\nteams. \\n Microservice: \\n• Deployed the microservice in Kubernetes service as containers \\n• Used CI/CD for deployment on Azure through pipeline yaml from DevOps. \\nWeb API : \\n• Deployed the WEB API on On-Premise Server for loading the updated records \\n• Hosted on IIS. \\nConsole Application: \\n• Configured the console application on Tas Scheduler for Data Sync Process form UI to \\nSurcharge Database. \\n \\nResponsibilities:  \\n• Analyzed business requirements and assessed the existing setup. \\n• Primarily focused on performance optimization, ensuring efficiency for multiple teams. \\n• Completed development within the estimated timeframe. • Adopted a monolithic architecture for the microservice. \\n• Evaluated scenarios to implement design principles and industry standards. \\n• Contributed to requirement gathering, design, development, and testing phases. \\n• Delivered high-quality results on time, meeting client expectations. \\n \\n \\n2. Pricing Application – Web Application \\nEnvironment : C# ASP.Net MVC and SQL Server 2019 \\nDescription : \\nA finance pricing application is a software solution designed to manage, analyze, and \\noptimize financial pricing strategie s. It helps businesses track market trends, set \\ncompetitive pricing, and ensure profitability while complying with financial regulations. \\n \\nKey Features: \\n• Dynamic Pricing Models:  Automate pricing adjustments based on market conditions, \\ncustomer demand, and competitor analysis. \\n• Risk Assessment Tools:  Evaluate financial risks associated with pricing strategies and \\nmitigate potential losses. \\n• Cost Analysis & Margin Optimization:  Calculate costs and profitability to ensure \\nsustainable pricing decisions. \\n• Real-time Data Insights: Access live financial metrics to make informed pricing decisions. \\n• Regulatory Compliance: Ensure pricing strategies align with financial and legal guidelines. \\n \\nResponsibilities:   \\n• Analyze and comprehend business and system requirements to ensure effective \\napplication development. \\n• Develop applications within stringent deadlines, tailored to meet customer needs. \\n• Deliver high-quality applications for both quality assurance and production environments. \\n• Estimate work effort required for project completion and resource allocation. \\n• Engage in requirement gathering, system design, development, and testing phases. • Participate in process -related activities, including documentation, resource billing, and \\nauditing. \\n• Ensure timely and high-quality deliverables that meet client expectations. \\n• Provide daily updates through Scrum meetings and status calls.  \\n \\n3. TAX APPLICATION – BLAZOR APPLICATION \\nEnvironment : .net core ,Blazor and Web API \\nDescription : \\nThis application is  developed for managing the user who initially becomes a member \\nthrough registration then takes it to CRM related things and complete s all the process. All \\nregistered users would be maintained by its appropriate Reps. Finally, the application will \\nhelpful for user in minimizing their taxes. \\n   Azure Functions to support the tax application -  \\na. Upload Files To My Pay – Timer Trigger  \\nb. Retrieve From Poller - Queue Trigger  \\nc. Renewal Engine – Timer Trigger \\nResponsibilities:  \\n• Understanding the business and system requirements provided. \\n• Developing application within estimated time based on demand. \\n• Followed design principle and used design patterns. \\n• Understanding the scenario and implementing the design and the standards \\n• Involved in requirement gathering, design, and development and testing phase  \\n• Provided quality and on-time delivery to the client. \\n• Task Status updation. \\n \\n4. MULTIVENDOR API AZURE FUNCTION -  SERVICE BUS TOPIC TRIGGER \\n \\nEnvironment :  .net core Function App  \\n \\n Description: \\nCreated Service Bus topic Triggers Function App for triggering the wrapped function and finally \\nloads the data once the message gets received in subscribed topic. This has been deployed by \\nCI/CD Pipeline from Azure DevOps to Azure Portal \\nResponsibilities: \\n• Leading Team of and developing Modules  \\n• Understanding the business and system requirements provided. \\n• Developing application within tight deadlines based on customer’s needs. \\n• Delivering applications with high quality in quality assurance and production \\nenvironments. \\n• Responsible for estimating work effort for project. \\n• Involved in requirement gathering, design, and development and testing phase. \\n• Involved in process activities (Documentation, Resource Billing and auditing). \\n• Provided quality and on-time delivery to the client. \\n• Daily Scrum and status call update. \\n \\n5. MULTIVENDOR SOURCE CONSOLE APPLICATION  \\nEnvironment:   .Net Core Windows App \\n \\nDescription: \\n Vendor Multisource Application developed which calls the API and extracts the data by \\nmapping each properties using Auto Mapper with validations and finally loads the data in table. \\nThis application will be configured and called by ThruPut Task Manager Windows Application \\nResponsibilities:  \\n• Understanding the business and system requirements provided. \\n• Developing application within tight deadlines based on customer’s needs. \\n• Delivering applications with high quality in quality assurance and production \\nenvironments. \\n• Responsible for estimating work effort for project. \\n• Involved in requirement gathering, design, and development and testing phase. • Involved in process activities (Documentation, Resource Billing and auditing). \\n• Provided quality and on-time delivery to the client. \\n \\n6. THRUPUT DASHBOARD - WEB IN MVC AND ANGULAR \\n \\nEnvironment:   Asp.Net, C# and SQL Server 2015 \\nDescription: \\nThis application is used to see the Supply Demand Reports for AFI, Wanek, Wanvog and \\nMillennium and also helps the Thruput users to view the sites and plans based on the dates. It \\nhas number of pages with CRUD operation and report download \\nIt shows the plans which got run on the same day by default \\nResponsibilities: \\n• Managing and leading the project team. \\n• Recruiting project staffs. \\n• Reverse Engineered the existing critical flows in the application. \\n• Managing co-ordination of the partners and working groups engaged in project work. \\n• Detailed project planning and control including: \\n➢ Developing and maintaining a detailed project plan. \\n➢ Managing project deliverables in line with the project plan. \\n➢ Maintaining billing for the project resources. \\n➢ Managing project scope and change control and escalating issues where necessary. \\n➢ Monitoring project progress and performance. \\n➢ Providing status reports to the project sponsor. \\n  \\n7. THRUPUT TASK MANAGER – WINDOWS APPLICATION \\n \\nEnvironment : .net windows Application and WCF Service  \\nDescription: Thruput task manager application is used to run various tasks for the individual sites and plans. \\nDivision refers to the BEDDING,  DOM and UPHOLSTERY. Plants would be referring to various \\nindividual sites such as Advance, Arcadia, etc. \\nResponsibilities: \\n• Leading a consolidated team of 6 members. \\n• Understanding the business and system requirements provided. \\n• Developing application within tight deadlines based on customer’s needs. \\n• Delivering applications with high quality in quality assurance and production \\nenvironments. \\n• Responsible for estimating work effort for project. \\n• Involved in requirement gathering, design, development and testing phase. \\n• Involved in process activities (Documentation, Resource Billing and auditing). \\n• Provided quality and on-time delivery to the client. \\n \\n8.  DATA SSIS PACKAGE -  SSDT \\n  \\nEnvironment : SSDT for ETL \\nDescription: \\nPurchasing data feed and Routing package is developed to extract data from AS400 environment \\nand populates it into the table in SQL Server  for scheduling purpose. Thruput Task manager calls \\nand execute these packages \\nResponsibilities: \\n• Leading a consolidated team of 5 members. \\n• Reverse Engineered the existing critical flows in the application. \\n• Understanding the business and system requirements provided. \\n• Developing application within tight deadlines based on customer’s needs. \\n• Delivering applications with high quality in quality assurance and production \\nenvironments. \\n• Responsible for estimating work effort for project. \\n• Involved in requirement gathering, design, development and testing phase. • Involved in process activities (Documentation, Resource Billing and auditing). \\n• Provided quality and on-time delivery to the client. \\n• Appreciated and Rewarded as part of recognition. \\n \\n \\n \\n9. VISUAL SCHEDULING -  MVC WEB APPLICATION \\n \\nEnvironment : ASP.Net MVC and Web API  \\nDescription: \\n The application is developed with MVC pages. Data is accessed from the combination of \\nAS400 environment and Sql Server. The Web  API service calls used to access the data from the \\ndatabases. Basically it will give an overall idea of amount of raw materials and other \\nmanufacturing things in warehouses based on environments to the user. \\nResponsibilities: \\n• Leading a consolidated team of 4 members. \\n• Understanding the business and system requirements provided. \\n• Developing application within tight deadlines based on customer’s needs. \\n• Delivering applications with high quality in quality assurance and production \\nenvironments. \\n• Responsible for estimating work effort for project. \\n• Involved in requirement gathering, design, and development and testing phase. \\n• Involved in process activities (Documentation, Resource Billing and auditing). \\n• Provided quality and on-time delivery to the client. \\n \\n10.  CNC DATA MANAGEMENT – WINDOWS APPLICATION \\n \\nEnvironment : C#. Net, SQL Server 2012 \\n \\n Description: \\n This system is used to manage the files across the local networks and read the machine \\ninstructions from the files and pass it to CNC Machine which is used to furnish the wood as per \\nthe instructions which we have sent and it is used for fabrication process. \\nResponsibilities: \\n• Reverse Engineered the existing critical flows in the application. \\n• Developed artefact and delivered on time and with high quality. \\n• Developed these critical enhancements for Verification and Pre-Verification screens. \\n• Co-ordinating the production support team in resolving complex issues on some of the \\ncritical flows in the application.  \\n \\n11.  MARKETING CRM (MARKETING CUSTOMER RELATIONSHIP MANAGEMENT) \\n \\nEnvironment : ASP.Net MVC, SQL Server 2012 \\n \\n Description: \\n A Marketing CRM (Customer Relationship Management)  system is designed to help businesses \\nmanage and optimize their marketing efforts by organizing customer interactions, tracking leads, \\nand automating engagement strategies. It enables companies to build strong relationships, \\nimprove campaign performance, and drive revenue growth. \\nKey Features: \\n• Lead & Contact Management: Store and organize customer data to personalize marketing \\noutreach. \\n• Campaign Automation:  Automate email campaigns, SMS marketing, and social media \\nengagement. \\n• Analytics & Insights: Track customer behavior and campaign performance using real -time \\ndata. \\n• Sales & Marketing Integration:  Align sales and marketing efforts to nurture leads and \\nboost conversions. • Customer Segmentation:  Categorize audiences for tailored messaging and targeted \\ncampaigns. \\n• Multi-Channel Engagement: \\n \\n \\nResponsibilities: \\n   \\n• Worked closely with product owners to understand project needs and requirements. \\n• Designed user interface elements and screens using technologies like ASP.NET, JavaScript, \\nand jQuery. \\n• Applied object-oriented programming to create reusable components for handling data. \\n• Used LINQ queries to efficiently retrieve and modify information from databases. \\n• Optimized database performance by creating indexes for faster searches. \\n• Enhanced user experience with interactive features using jQuery. \\n• Led system and integration testing to ensure smooth functionality. \\n• Managed updates and improvements to keep the application running effectively. \\n \\n ')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "from langchain_neo4j import Neo4jVector\n",
    "\n",
    "#define tool\n",
    "vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedder,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=\"text_embeddings\",\n",
    "    node_label=\"Person\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")\n",
    "\n",
    "def search_documents(search_prompt:str) -> List[Dict[str, Any]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieval knowledge by searching people resumes\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = vector_store.similarity_search(search_prompt, k=5)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return [{\"error\":str(e)}]\n",
    "#test\n",
    "search_documents(\"\")[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14114ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build adk agent with Azure OpenAI + Neo4j MCP\n",
    "from neo4j_graphrag.llm import AzureOpenAILLM\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "from google.adk.agents import Agent\n",
    "from AgentRunner import AgentRunner  # same as you had\n",
    "from google.genai.types import Part, UserContent\n",
    "\n",
    "\n",
    "\n",
    "# ✅ model parameters for consistency\n",
    "\n",
    "#llm = LiteLlm()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271f5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session started successfully with ID: 1eeca815-9d9a-4993-af8f-b6f6a457ae64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "AGENT_INSTRUCTION = \"\"\"\n",
    "You are a human resources assistant who helps with exploring the resumes \n",
    "of different candidates at Shantanu's Systems.\n",
    "\n",
    "You can access the resumes of Shantanu's System employees directly\n",
    "using your `search_documents` tool. You may access this multiple\n",
    "times as needed to get the information for a user.\n",
    "\n",
    "When responding to the user:\n",
    "- if your response includes people, include their names, links, roles, and any other relevant information.\n",
    "- You must explain your retrieval logic and where the data came from.\n",
    "- You must say exactly how relevance, similarity, etc. was inferred during search.\n",
    "- You must explore all resumes documents thoroughly to find the most relevant information.\n",
    "- You must respond on the basis of the retrieved information and not make assumptions.\n",
    "\"\"\"\n",
    "\n",
    "# ✅ build agent with Azure OpenAI\n",
    "search_agent = Agent(\n",
    "    name=\"search_agent\",\n",
    "    #model=llm,   # << replaced LiteLlm with AzureOpenAILLM instance\n",
    "    description=\"Agent to access employee knowledge stored in documents\",\n",
    "    instruction=AGENT_INSTRUCTION,\n",
    "    tools=[search_documents]\n",
    ")\n",
    "\n",
    "# ✅ run agent\n",
    "runner = AgentRunner(app_name='search_agent', user_id='Shantanu Sharma', agent=search_agent)\n",
    "await runner.start_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8af003bf9f8fc",
   "metadata": {},
   "source": [
    "### How many Python developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa76bd258c7958ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T11:58:20.856735Z",
     "start_time": "2025-08-11T11:58:14.979208Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")  # adjust if needed\n",
    "\n",
    "async def run_with_token_count(runner, prompt_text):\n",
    "    # Count prompt tokens\n",
    "    prompt_tokens = len(tokenizer.encode(prompt_text))\n",
    "\n",
    "    # Run your runner with the prompt\n",
    "    result = await runner.run(prompt_text)\n",
    "\n",
    "    # Extract answer text (adjust if result format differs)\n",
    "    answer_text = result if isinstance(result, str) else getattr(result, \"content\", str(result))\n",
    "\n",
    "    # Count completion tokens\n",
    "    completion_tokens = len(tokenizer.encode(answer_text))\n",
    "\n",
    "    # Print outputs with tokens info\n",
    "    total_tokens = prompt_tokens + completion_tokens\n",
    "    print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "    print(f\"Completion tokens: {completion_tokens}\")\n",
    "    print(f\"Total tokens: {total_tokens}\\n\")\n",
    "    print(\"Answer:\\n\", answer_text)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Usage\n",
    "#Provide an overall summary of the talent pool in terms of skills, domains, and experience.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39774a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await run_with_token_count(runner, \"Compare the profiles of two candidates with vector database experience.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de947ee6b78650f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T11:59:11.953899Z",
     "start_time": "2025-08-11T11:59:11.949395Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
