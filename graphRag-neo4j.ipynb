{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06890560",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neo4j-graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c839a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neo4j-graphrag[openai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd527822",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffd289c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This example demonstrates how to use SimpleKGPipeline with automatic schema extraction\n",
    "from a PDF file. When no schema is provided to SimpleKGPipeline, automatic schema extraction\n",
    "is performed using the LLM.\n",
    "\n",
    "Note: This example requires an OpenAI API key to be set in the .env file.\n",
    "\"\"\"\n",
    "\n",
    "import neo4j\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.generation.graphrag import GraphRAG\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.embeddings import AzureOpenAIEmbeddings\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "from neo4j_graphrag.llm import AzureOpenAILLM\n",
    "from neo4j_graphrag.experimental.components.text_splitters.fixed_size_splitter import FixedSizeSplitter\n",
    "from neo4j_graphrag.experimental.pipeline import Pipeline\n",
    "from neo4j_graphrag.schema import get_schema\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ef81432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.generation.graphrag import GraphRAG\n",
    "\n",
    "\n",
    "async def run_kg_pipeline_with_auto_schema() -> None:\n",
    "    \"\"\"Run the SimpleKGPipeline with automatic schema extraction from a PDF file.\"\"\"\n",
    "\n",
    "    # Define Neo4j connection\n",
    "NEO4J_URI=\"neo4j+ssc://efea2c90.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"7V18VY7NXa1QQl06JD7_FONhdeqSap_7pUMBTgg-o3A\"\n",
    "NEO4J_DATABASE=\"neo4j\"\n",
    "AURA_INSTANCEID=\"efea2c90\"\n",
    "AURA_INSTANCENAME=\"Instance01\"\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    " \n",
    "AUTH = (NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "with GraphDatabase.driver(NEO4J_URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba595511",
   "metadata": {},
   "source": [
    "Reading all the pdf files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a75f2ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('C:/Users/shant/OneDrive/Desktop/GraphRAGresume/input/Abhishek_Nandgadkar.pdf')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_path = Path(r\"C:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\input\")\n",
    "\n",
    "# Collect all PDF files in that folder\n",
    "pdf_files = list(folder_path.glob(\"*.pdf\"))\n",
    "print(pdf_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9c134ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: C:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\input\\Abhishek_Nandgadkar.pdf\n",
      "Extracted Text length: 2902\n",
      "Number of chunks: 1\n",
      "\n",
      "Chunk 1 (length 2901):\n",
      "Abhishek Nandgadkar\n",
      "Data Engineer | Data Analyst | SQL | Python | Spark | Airflow | ADF | Big Data | Real-Time Data Processing | Kafka\n",
      "| Cloud Services (AWS, GCP, Azure) | Building High-Performance ETL Pipelines\n",
      "abhisheknandgadkar41@gmail.com 6360952485 Belgaum, Karnataka\n",
      "abhishek-nandgadkar-4760a429b\n",
      "WORK EXPERIENCE\n",
      "Nubax Data Labs\n",
      "Data Engineer 02/2023 - Present\n",
      "‚Ä¢ Designed scalable ETL pipelines for large datasets, optimizing performance.\n",
      "‚Ä¢ Built efficient, reliable data workflows using Spark and AWS Glue, leveraging Glue jobs for schema\n",
      "management and batch data processing.\n",
      "‚Ä¢ Utilized Databricks for distributed data processing, enabling scalable, high-performance analytics and\n",
      "reducing processing times for complex data transformations.\n",
      "‚Ä¢ Conducted data profiling and validation to meet data governance and quality standards.\n",
      "‚Ä¢ Implemented CI/CD pipelines for data workflows, reducing deployment times and improving reliability.\n",
      "‚Ä¢ Created and maintained Grafana dashboards for real-time m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to extract full text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    full_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            full_text += page.extract_text() + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "# Folder containing PDF files\n",
    "folder_path = \"C:\\\\Users\\\\shant\\\\OneDrive\\\\Desktop\\\\GraphRAGresume\\\\input\"\n",
    "\n",
    "# List all PDF files in the folder\n",
    "pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "# Initialize RecursiveCharacterTextSplitter once\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"\\nProcessing file: {pdf_path}\")\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    print(\"Extracted Text length:\", len(full_text))\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    for i, chunk in enumerate(chunks[:5], 1):\n",
    "        print(f\"\\nChunk {i} (length {len(chunk)}):\\n{chunk[:1000]}\")  # Display first 1000 chars of chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09980214",
   "metadata": {},
   "source": [
    "Initialising LLM ,Embedder and Driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c491766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.generation.graphrag import GraphRAG\n",
    "\n",
    "# Neo4j connection details\n",
    "NEO4J_URI = \"neo4j+ssc://efea2c90.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"7V18VY7NXa1QQl06JD7_FONhdeqSap_7pUMBTgg-o3A\"\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6cf049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\.venv\\Lib\\site-packages\\neo4j\\_sync\\driver.py:542: ResourceWarning: unclosed  Neo4jDriver: <neo4j._sync.driver.Neo4jDriver object at 0x00000139662A2780>.\n",
      "  _unclosed_resource_warn(self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\.venv\\Lib\\site-packages\\neo4j\\_sync\\driver.py:547: DeprecationWarning: Relying on Driver's destructor to close the session is deprecated. Please make sure to close the session. Use it as a context (`with` statement) or make sure to call `.close()` explicitly. Future versions of the driver will not close drivers automatically.\n",
      "  _deprecation_warn(\n"
     ]
    }
   ],
   "source": [
    "    # Define LLM parameters\n",
    "\n",
    "llm_model_params = {\n",
    "        \"max_tokens\": 5000,\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"temperature\": 0,  # Lower temperature for more consistent output\n",
    "    }\n",
    "\n",
    "    # Initialize the Neo4j driver\n",
    "driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "    # Create the LLM instance\n",
    "\n",
    "llm = AzureOpenAILLM(\n",
    "    #model_name=\"gpt-4o-intern\",\n",
    "    # model_name=\"gpt-4o\",  # This should match your Azure deployment name for the LLM\n",
    "    #model_name =  # This should match your Azure deployment name for the LLM\n",
    "    #azure_endpoint=  # update with your endpoint\n",
    "    #api_version=  # update appropriate version\n",
    "    #api_key=  # api_key is optional and can also be set with OPENAI_API_KEY env var\n",
    "    #model_params=llm_model_params,\n",
    "    )\n",
    "\n",
    "    # Create the embedder instance\n",
    "\n",
    "embedder = AzureOpenAIEmbeddings(\n",
    "        #model=\n",
    "        # This should match your Azure deployment name for the embedding model\n",
    "        #api_key=  # Your Azure OpenAI API key\n",
    "        #azure_endpoint=  # update with your endpoint\n",
    "        #api_version=  # update appropriate version\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9027c14",
   "metadata": {},
   "source": [
    "Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd6b9866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\.venv\\Lib\\site-packages\\neo4j\\_sync\\driver.py:542: ResourceWarning: unclosed  Neo4jDriver: <neo4j._sync.driver.Neo4jDriver object at 0x0000013964E4E5A0>.\n",
      "  _unclosed_resource_warn(self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\input\\Abhishek_Nandgadkar.pdf\n",
      "Result: run_id='7dd1bfee-da2c-4bee-820b-7548f193a425' result={'resolver': {'number_of_nodes_to_resolve': 27, 'number_of_created_nodes': 14}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "        # Create a SimpleKGPipeline instance without providing a schema\n",
    "        # This will trigger automatic schema extraction\n",
    "kg_builder = SimpleKGPipeline(\n",
    "llm=llm,\n",
    "driver=driver,\n",
    "embedder=embedder,\n",
    "from_pdf=True,\n",
    ")\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "    pdf_result=await kg_builder.run_async(file_path=str(pdf_file))\n",
    "    print(f\"Result: {pdf_result}\")\n",
    "    # Close connections\n",
    "#await llm.async_client.close()\n",
    "#driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def main() -> None:\n",
    "    # Run the pipeline\n",
    "    await run_kg_pipeline_with_auto_schema()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    await(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dde16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI=\"neo4j+ssc://efea2c90.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"7V18VY7NXa1QQl06JD7_FONhdeqSap_7pUMBTgg-o3A\"\n",
    "NEO4J_DATABASE=\"neo4j\"\n",
    "AURA_INSTANCEID=\"efea2c90\"\n",
    "AURA_INSTANCENAME=\"Instance01\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "captured_cypher_queries = []\n",
    "\n",
    "# Wrap the run method of session to capture queries\n",
    "original_session_run = driver.session().run\n",
    "\n",
    "def wrapped_run(self, query, parameters=None, **kwargs):\n",
    "    captured_cypher_queries.append(query)\n",
    "    return original_session_run(query, parameters or {}, **kwargs)\n",
    "\n",
    "# Patch the session run method\n",
    "from types import MethodType\n",
    "\n",
    "def patch_driver_run(driver):\n",
    "    original_session = driver.session\n",
    "\n",
    "    def new_session(*args, **kwargs):\n",
    "        session = original_session(*args, **kwargs)\n",
    "        session.run = MethodType(wrapped_run, session)\n",
    "        return session\n",
    "\n",
    "    driver.session = new_session\n",
    "\n",
    "patch_driver_run(driver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77b977",
   "metadata": {},
   "source": [
    "Creation of Indexes in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "\n",
    "INDEX_NAME = \"vector-index-name\"\n",
    "NEO4J_URI = \"neo4j+ssc://efea2c90.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"7V18VY7NXa1QQl06JD7_FONhdeqSap_7pUMBTgg-o3A\"\n",
    "\n",
    "# Connect to the Neo4j database\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "# Create the index (None return is normal)\n",
    "res = create_vector_index(\n",
    "    driver,\n",
    "    INDEX_NAME,\n",
    "    label=\"Chunk\",\n",
    "    embedding_property=\"embedding\",\n",
    "    dimensions=1536,\n",
    "    similarity_fn=\"cosine\",\n",
    ")\n",
    "\n",
    "print(f\"Index creation function returned: {res}\")\n",
    "\n",
    "# IMPORTANT: Verify if index was actually created\n",
    "print(\"üîç Verifying index creation...\")\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"SHOW INDEXES WHERE name = $name\", name=INDEX_NAME)\n",
    "    index_info = result.single()\n",
    "    \n",
    "    if index_info:\n",
    "        details = dict(index_info)\n",
    "        print(f\"‚úÖ SUCCESS! Index '{INDEX_NAME}' exists:\")\n",
    "        print(f\"   Type: {details.get('type')}\")\n",
    "        print(f\"   State: {details.get('state')}\")\n",
    "        print(f\"   Properties: {details.get('properties')}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Index '{INDEX_NAME}' was not created\")\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.schema import get_schema\n",
    "from neo4j import GraphDatabase\n",
    " \n",
    "schema = get_schema(driver, database=\"neo4j\")\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa67a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    \n",
    "    entities = session.run(\"MATCH (n) RETURN count(n) as entity_count\").single()[\"entity_count\"]\n",
    "    relationships = session.run(\"MATCH ()-[r]->() RETURN count(r) as rel_count\").single()[\"rel_count\"]\n",
    "    \n",
    "    print(f\"Total entities: {entities}, Total relationships: {relationships}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cf9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_query = \"\"\"\n",
    "MATCH (n)\n",
    "WHERE NOT n:Document AND NOT n:Chunk\n",
    "WITH collect(n) AS nodes\n",
    "UNWIND nodes AS n\n",
    "MATCH p=(n)-[r]-(m)\n",
    "WHERE NOT m:Document AND NOT m:Chunk\n",
    "WITH DISTINCT p, labels(n) AS labels\n",
    "RETURN p,\n",
    "CASE \n",
    "    WHEN 'Person' IN labels THEN '#ff6b6b'       // red\n",
    "    WHEN 'Company' IN labels THEN '#4dabf7'      // blue\n",
    "    WHEN 'Location' IN labels THEN '#51cf66'     // green\n",
    "    WHEN 'Skill' IN labels THEN '#f59f00'        // orange\n",
    "    ELSE '#cccccc'                               // gray default\n",
    "END AS color\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    results = session.run(cypher_query)\n",
    "    for record in results:\n",
    "        path = record[\"p\"]\n",
    "        # path is a Path object: nodes and relationships can be extracted\n",
    "        nodes = [dict(node) for node in path.nodes]\n",
    "        relationships = [dict(rel) for rel in path.relationships]\n",
    "        print(f\"Path nodes: {nodes}\")\n",
    "        print(f\"Path relationships: {relationships}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03856011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "import tiktoken\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    " \n",
    "# # --- configure ---\n",
    "# NEO4J_URI = \"bolt://localhost:7687\"\n",
    "# NEO4J_USER = \"neo4j\"\n",
    "# NEO4J_PASSWORD = \"password\"\n",
    " \n",
    "# Choose tokenizer/model\n",
    "#MODEL_NAME = \"gpt-4o-mini\"  \n",
    "try:\n",
    "    enc = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "except KeyError:\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    " \n",
    "def my_converter(o):\n",
    "    if isinstance(o, datetime):\n",
    "        return o.isoformat()\n",
    "    # Add similar check for neo4j.time.DateTime or custom types as needed\n",
    "    return str(o)\n",
    "\n",
    "def serialize_value(v):\n",
    "    \"\"\"Stable JSON-like serialization for property values.\"\"\"\n",
    "    return json.dumps(v, ensure_ascii=True, sort_keys=True, default=my_converter)\n",
    "\n",
    "    #return json.dumps(v, ensure_ascii=True, sort_keys=True)\n",
    " \n",
    "def node_to_text(node):\n",
    "    \"\"\"Stable string for a node: labels + id + properties.\"\"\"\n",
    "    labels = \":\".join(sorted(list(node.labels))) if hasattr(node, \"labels\") else \"\"\n",
    "    props = dict(node.items())\n",
    "    prop_str = \", \".join(f'{k}={serialize_value(props[k])}' for k in sorted(props.keys()))\n",
    "    elem_id = getattr(node, \"element_id\", None) or getattr(node, \"id\", None)\n",
    "    return f\"({labels})[{elem_id}] {{{prop_str}}}\"\n",
    " \n",
    " \n",
    "def rel_to_text(rel):\n",
    "    \"\"\"Stable string for a relationship: type + ids + properties.\"\"\"\n",
    "    rel_type = rel.type if hasattr(rel, \"type\") else \"\"\n",
    "    props = dict(rel.items())\n",
    "    prop_str = \", \".join(f'{k}={serialize_value(props[k])}' for k in sorted(props.keys()))\n",
    "    start_id = getattr(rel, \"start_node_element_id\", None) or getattr(rel, \"start\", None)\n",
    "    end_id = getattr(rel, \"end_node_element_id\", None) or getattr(rel, \"end\", None)\n",
    "    elem_id = getattr(rel, \"element_id\", None) or getattr(rel, \"id\", None)\n",
    "    return f\"[:{rel_type}][{elem_id}] (start={start_id}, end={end_id}) {{{prop_str}}}\"\n",
    " \n",
    " \n",
    "def count_tokens(text):\n",
    "    return len(enc.encode(text))\n",
    " \n",
    " \n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(AUTH))\n",
    " \n",
    "# ---- NODES ----\n",
    "total_node_tokens = 0\n",
    "total_nodes = 0\n",
    "by_label_tokens = defaultdict(int)\n",
    "by_label_count = defaultdict(int)\n",
    " \n",
    "with driver.session() as session:\n",
    "    results = session.run(\"MATCH (n) RETURN n\")\n",
    "    for record in results:\n",
    "        n = record[\"n\"]\n",
    "        text = node_to_text(n)\n",
    "        tokens = count_tokens(text)\n",
    " \n",
    "        total_node_tokens += tokens\n",
    "        total_nodes += 1\n",
    " \n",
    "        label_key = tuple(sorted(getattr(n, \"labels\", ())))\n",
    "        by_label_tokens[label_key] += tokens\n",
    "        by_label_count[label_key] += 1\n",
    " \n",
    "# ---- RELATIONSHIPS ----\n",
    "total_rel_tokens = 0\n",
    "total_rels = 0\n",
    "by_type_tokens = defaultdict(int)\n",
    "by_type_count = defaultdict(int)\n",
    " \n",
    "with driver.session() as session:\n",
    "    results = session.run(\"MATCH ()-[r]->() RETURN r\")\n",
    "    for record in results:\n",
    "        r = record[\"r\"]\n",
    "        text = rel_to_text(r)\n",
    "        tokens = count_tokens(text)\n",
    " \n",
    "        total_rel_tokens += tokens\n",
    "        total_rels += 1\n",
    " \n",
    "        rel_type = r.type if hasattr(r, \"type\") else \"UNKNOWN\"\n",
    "        by_type_tokens[rel_type] += tokens\n",
    "        by_type_count[rel_type] += 1\n",
    " \n",
    "driver.close()\n",
    " \n",
    "# ---- Print Summary ----\n",
    "print(\"=== Nodes ===\")\n",
    "print(f\"Total nodes: {total_nodes}\")\n",
    "print(f\"Total node tokens: {total_node_tokens}\")\n",
    "for labels, tok in sorted(by_label_tokens.items(), key=lambda x: -x[1]):\n",
    "    label_name = \":\".join(labels) if labels else \"(no labels)\"\n",
    "    avg = tok / by_label_count[labels]\n",
    "    print(f\"- {label_name}: nodes={by_label_count[labels]}, tokens={tok}, avg_tokens_per_node={avg:.2f}\")\n",
    " \n",
    "print(\"\\n=== Relationships ===\")\n",
    "print(f\"Total relationships: {total_rels}\")\n",
    "print(f\"Total relationship tokens: {total_rel_tokens}\")\n",
    "for rel_type, tok in sorted(by_type_tokens.items(), key=lambda x: -x[1]):\n",
    "    avg = tok / by_type_count[rel_type]\n",
    "    print(f\"- {rel_type}: rels={by_type_count[rel_type]}, tokens={tok}, avg_tokens_per_rel={avg:.2f}\")\n",
    " \n",
    "print(\"\\n=== Overall ===\")\n",
    "print(f\"Total tokens (nodes + relationships): {total_node_tokens + total_rel_tokens}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89785267",
   "metadata": {},
   "source": [
    "token calculation for entities and relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6945d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_template = '''Answer the Question using the following Context. Only respond with information mentioned in the Context. Do not inject any speculative information not mentioned.\n",
    "\n",
    "# Question:\n",
    "{query_text}\n",
    "\n",
    "# Context:\n",
    "{context}\n",
    "\n",
    "# Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a3d71",
   "metadata": {},
   "source": [
    "vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "\n",
    "create_vector_index(driver, \n",
    "                    name=\"vector-index-name\", \n",
    "                    label=\"Chunk\", \n",
    "                    embedding_property=\"embedding\", \n",
    "                    dimensions=1536,  \n",
    "                    similarity_fn=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "\n",
    "vector_retriever = VectorRetriever(\n",
    "   driver,\n",
    "   index_name=\"vector-index-name\",\n",
    "   embedder=embedder,\n",
    "   return_properties=[\"text\", \"chunk_text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "vector_res = vector_retriever.get_search_results(query_text = \"Who all have worked with vector Databases?\",top_k=5)\n",
    "for i in vector_res.records: print(\"====n\" + json.dumps(i.data(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e679b3a",
   "metadata": {},
   "source": [
    "VectorCypherRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa39e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorCypherRetriever\n",
    "\n",
    "vc_retriever = VectorCypherRetriever(\n",
    "    driver,\n",
    "    index_name=\"vector-index-name\",\n",
    "    embedder=embedder,\n",
    "    retrieval_query=\"\"\"\n",
    "//1) Go out 2-3 hops in the entity graph and get relationships\n",
    "\n",
    "WITH node AS chunk\n",
    "MATCH (chunk)<-[:FROM_CHUNK]-()-[relList:!FROM_CHUNK]-{1,2}()\n",
    "UNWIND relList AS rel\n",
    " \n",
    "//2) collect relationships and text chunks\n",
    "WITH collect(DISTINCT chunk) AS chunks,\n",
    " collect(DISTINCT rel) AS rels\n",
    " \n",
    "//3) format and return context\n",
    "RETURN '=== text ===n' + apoc.text.join([c in chunks | c.text], 'n---n') + 'nn=== kg_rels ===n' +\n",
    " apoc.text.join([r in rels | startNode(r).name + ' - ' + type(r) + '(' + coalesce(r.details, '') + ')' +  ' -> ' + endNode(r).name ], 'n---n') AS info\n",
    "\"\"\"\n",
    ")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_res = vc_retriever.get_search_results(query_text = \"summarise me the resume of ALOK MISHRA?\", top_k=1)\n",
    "\n",
    "# print output\n",
    "kg_rel_pos = vc_res.records[0]['info'].find('nn=== kg_rels ===n')\n",
    "print(\"# Text Chunk Context:\")\n",
    "print(vc_res.records[0]['info'][:kg_rel_pos])\n",
    "print(\"# KG Context From Relationships:\")\n",
    "print(vc_res.records[0]['info'][kg_rel_pos:])\n",
    "\n",
    "# Assume vector_retriever and embedder are already initialized as shown earlier\n",
    "\n",
    "# Embed the query into a vector\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962552c",
   "metadata": {},
   "source": [
    "Text2Cypher Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j\n",
    "from neo4j_graphrag.retrievers import Text2CypherRetriever\n",
    "\n",
    "\n",
    "llm_model_params = {\n",
    "        \"max_tokens\": 2000,\n",
    "        \"response_format\": {\"type\": \"text\"},\n",
    "        \"temperature\": 0,  # Lower temperature for more consistent output\n",
    "    }\n",
    "\n",
    "    # Initialize the Neo4j driver\n",
    "driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "llm = AzureOpenAILLM(\n",
    "    #model_name=\"gpt-4o-intern\",\n",
    "    # model_name=\"gpt-4o\",  # This should match your Azure deployment name for the LLM\n",
    "# (Optional) Provide user input/query pairs for the LLM to use as examples\n",
    ")\n",
    "\n",
    "with neo4j.GraphDatabase.driver(NEO4J_URI, auth=AUTH) as driver:\n",
    "    # Initialize the retriever\n",
    "    Text2Cypher_Retriever = Text2CypherRetriever(\n",
    "        driver,\n",
    "        llm=llm,\n",
    "        neo4j_schema=schema,\n",
    "        neo4j_database='neo4j',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e78001",
   "metadata": {},
   "source": [
    "Hybrid-Cypher Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1604e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_fulltext_index\n",
    "FULLTEXT_INDEX_NAME = \"fulltext_index\"\n",
    "\n",
    "driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=AUTH)\n",
    "create_fulltext_index(\n",
    "    driver, FULLTEXT_INDEX_NAME, label=\"Document\", node_properties=[\"textProperty\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d31cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import HybridCypherRetriever\n",
    "llm_model_params = {\n",
    "        \"max_tokens\": 2000,\n",
    "        \"response_format\": {\"type\": \"text\"},\n",
    "        \"temperature\": 0,  # Lower temperature for more consistent output\n",
    "    }\n",
    "llm = AzureOpenAILLM(\n",
    "    #model_name=\"gpt-4o-intern\",\n",
    "    # model_name=\"gpt-4o\",  # This should match your Azure deployment name for the LLM\n",
    ")\n",
    "retrieval_query = \"\"\"\n",
    "// Just return the main content without complex relationships\n",
    "WITH node\n",
    "RETURN \n",
    "    CASE \n",
    "        WHEN node.text IS NOT NULL THEN node.text\n",
    "        WHEN node.content IS NOT NULL THEN node.content  \n",
    "        WHEN node.description IS NOT NULL THEN node.description\n",
    "        WHEN node.name IS NOT NULL THEN 'Name: ' + node.name\n",
    "        ELSE 'Node ID: ' + elementId(node)\n",
    "    END AS info\n",
    "\"\"\"\n",
    "\n",
    "with neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "    # Initialize the retriever\n",
    "    HybridCypher_Retriever = HybridCypherRetriever(\n",
    "        driver=driver,\n",
    "        vector_index_name='vector-index-name',\n",
    "        fulltext_index_name='fulltext_index',\n",
    "        embedder=embedder,\n",
    "        retrieval_query=retrieval_query,\n",
    "        result_formatter=None,\n",
    "    )\n",
    "    # Perform the similarity search for a text query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e09b00",
   "metadata": {},
   "source": [
    "Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import HybridRetriever\n",
    "\n",
    "INDEX_NAME = \"vector-index-name\"\n",
    "FULLTEXT_INDEX_NAME = \"fulltext_index\"\n",
    "\n",
    "llm_model_params = {\n",
    "        \"max_tokens\": 2000,\n",
    "        #\"response_format\": {\"type\": \"json_object\"},\n",
    "        \"temperature\": 0,  # Lower temperature for more consistent output\n",
    "    }\n",
    "\n",
    "    # Initialize the Neo4j driver\n",
    "    # Create the LLM instance\n",
    "\n",
    "llm = AzureOpenAILLM(\n",
    "    #model_name=\"gpt-4o-intern\",\n",
    "    # model_name=\"gpt-4o\",  # This should match your Azure deployment name for the LLM\n",
    "    \n",
    "    )\n",
    "with neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "    # Initialize the retriever\n",
    "    Hybrid_Retriever = HybridRetriever(\n",
    "        driver=driver,\n",
    "        vector_index_name=\"vector-index-name\",\n",
    "        fulltext_index_name=\"fulltext_index\",\n",
    "        embedder=embedder,\n",
    "    )\n",
    "\n",
    "    # Perform the similarity search for a text query\n",
    "    # (retrieve the top 5 most similar nodes)\n",
    "    #query_text = \"Who all have worked with vector Databases\"\n",
    "    #print(Hybrid_Retriever.search(query_text=query_text, top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4fbbf",
   "metadata": {},
   "source": [
    "Instantiate the rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.generation.prompts import RagTemplate\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "\n",
    "# Define the template as a string\n",
    "template_str = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query_text}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the RagTemplate object\n",
    "rag_template = RagTemplate(\n",
    "    template=template_str,\n",
    "    expected_inputs=['query_text', 'context']\n",
    ")\n",
    "\n",
    "\n",
    "vector_retriever_rag  = GraphRAG(llm=llm, retriever=vector_retriever, prompt_template=rag_template)\n",
    "vector_cypher_retriever_rag = GraphRAG(llm=llm, retriever=vc_retriever, prompt_template=rag_template)\n",
    "Text2Cypher_Retriever_rag = GraphRAG(retriever=Text2Cypher_Retriever, llm=llm)\n",
    "HybridCypher_Retriever_rag = GraphRAG(retriever=HybridCypher_Retriever, llm=llm)\n",
    "Hybrid_Retriever_rag = GraphRAG(retriever=Hybrid_Retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0a670",
   "metadata": {},
   "source": [
    "Different Query Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveQueryTracker:\n",
    "    def __init__(self, driver):\n",
    "        self.driver = driver\n",
    "        self.tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "        self.reset_sessions()\n",
    "    \n",
    "    def reset_sessions(self):\n",
    "        self.query_tokens = 0\n",
    "        self.embedding_tokens = 0\n",
    "        self.retrieval_prompt_tokens = 0\n",
    "        self.retrieval_completion_tokens = 0\n",
    "        self.context_tokens = 0\n",
    "        self.final_answer_tokens = 0\n",
    "        self.start_time = datetime.utcnow()\n",
    "    \n",
    "    def count_tokens(self, text):\n",
    "        return len(self.tokenizer.encode(text or \"\"))\n",
    "    \n",
    "    def track_embedding(self, text):\n",
    "        tokens = self.count_tokens(text)\n",
    "        self.embedding_tokens += tokens\n",
    "        print(f\"üîç [QUERY EMBED] {tokens} tokens\")\n",
    "        return tokens\n",
    "    \n",
    "    def track_context(self, context):\n",
    "        tokens = self.count_tokens(context)\n",
    "        self.context_tokens += tokens\n",
    "        print(\"\\nüìÑ [RETRIEVED CONTEXT]\")\n",
    "        print(\"-\"*80)\n",
    "        print(context[:2000] + (\"...\" if len(context) > 2000 else \"\"))\n",
    "        print(\"-\"*80)\n",
    "        return tokens\n",
    "    \n",
    "    def track_llm(self, prompt, response):\n",
    "        prompt_tokens = self.count_tokens(prompt)\n",
    "        completion_tokens = 0\n",
    "        if isinstance(response, dict):\n",
    "            completion_tokens = response.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "            if completion_tokens == 0 and response.get(\"choices\"):\n",
    "                text_resp = response[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\") or response[\"choices\"][0].get(\"text\", \"\")\n",
    "                completion_tokens = self.count_tokens(text_resp)\n",
    "        elif isinstance(response, str):\n",
    "            completion_tokens = self.count_tokens(response)\n",
    "        elif hasattr(response, \"text\"):\n",
    "            completion_tokens = self.count_tokens(response.text)\n",
    "        else:\n",
    "            completion_tokens = self.count_tokens(str(response))\n",
    "        \n",
    "        self.retrieval_prompt_tokens += prompt_tokens\n",
    "        self.retrieval_completion_tokens += completion_tokens\n",
    "        print(f\"ü§ñ [LLM] prompt={prompt_tokens}, completion={completion_tokens}\")\n",
    "        return prompt_tokens, completion_tokens\n",
    "    \n",
    "    def print_summary(self, query, retriever_name, answer):\n",
    "        self.query_tokens = self.count_tokens(query)\n",
    "        self.final_answer_tokens = self.count_tokens(answer)\n",
    "        total_input = (\n",
    "            self.query_tokens +\n",
    "            self.embedding_tokens +\n",
    "            self.retrieval_prompt_tokens +\n",
    "            self.context_tokens\n",
    "        )\n",
    "        total_output = self.retrieval_completion_tokens + self.final_answer_tokens\n",
    "        grand_total = total_input + total_output\n",
    "        duration = (datetime.utcnow() - self.start_time).total_seconds()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"üìä QUERY TOKEN SUMMARY for {retriever_name}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Query tokens:           {self.query_tokens}\")\n",
    "        print(f\"Embedding tokens:       {self.embedding_tokens}\")\n",
    "        #print(f\"Retrieval LLM prompt:   {self.retrieval_prompt_tokens}\")\n",
    "        #print(f\"Retrieval LLM output:   {self.retrieval_completion_tokens}\")\n",
    "        print(f\"Context tokens:         {self.context_tokens}\")\n",
    "        print(f\"Final answer tokens:    {self.final_answer_tokens}\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Total input tokens:     {total_input}\")\n",
    "        print(f\"Total output tokens:    {total_output}\")\n",
    "        print(f\"Grand total tokens:     {grand_total}\")\n",
    "        print(f\"Duration (s):           {duration:.2f}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "query_tracker = ComprehensiveQueryTracker(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"give me education details of Abhishek Nandgadkar?\"\n",
    "\n",
    "def run_and_log_with_context(retriever_name, retriever, prompt_template=None):\n",
    "    query_tracker.reset_sessions()\n",
    "\n",
    "    # 1) Raw context retrieval (skip for Text2Cypher)\n",
    "    if hasattr(retriever, \"get_search_results\") and retriever_name != \"Text2CypherRetriever\":\n",
    "        try:\n",
    "            raw = retriever.get_search_results(query_text=query_text, top_k=5)\n",
    "            print(f\"üîç Retrieved {len(raw.records)} records from {retriever_name}\")\n",
    "            texts = []\n",
    "            for rec in raw.records:\n",
    "                data = rec.data()\n",
    "                if \"node\" in data and isinstance(data[\"node\"], dict) and \"text\" in data[\"node\"]:\n",
    "                    texts.append(data[\"node\"][\"text\"])\n",
    "                else:\n",
    "                    for v in data.values():\n",
    "                        if isinstance(v, str) and v.strip():\n",
    "                            texts.append(v)\n",
    "            \n",
    "            if texts:\n",
    "                context = \"\\n---\\n\".join(texts)\n",
    "                \n",
    "                # PRINT THE ACTUAL CONTEXT BEING TOKENIZED\n",
    "                print(f\"\\nüìÑ CONTEXT CONTENT for {retriever_name}:\")\n",
    "                print(\"=\" * 80)\n",
    "                print(context[:2000] + \"...\" if len(context) > 2000 else context)  # Show first 2000 chars\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"Full context length: {len(context)} characters\")\n",
    "                \n",
    "                query_tracker.track_context(context)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No context extracted for {retriever_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Context fetch failed for {retriever_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Skipping raw context fetch for {retriever_name}\")\n",
    "\n",
    "    # 2) Perform RAG search\n",
    "    rag = (\n",
    "        GraphRAG(llm=llm, retriever=retriever, prompt_template=prompt_template)\n",
    "        if prompt_template\n",
    "        else GraphRAG(llm=llm, retriever=retriever)\n",
    "    )\n",
    "    \n",
    "    if retriever_name == \"Text2CypherRetriever\":\n",
    "        result = rag.search(query_text)\n",
    "    else:\n",
    "        result = rag.search(query_text, retriever_config={\"top_k\": 5})\n",
    "\n",
    "    # 3) Print answer and tokens summary\n",
    "    print(f\"\\n===== {retriever_name} =====\")\n",
    "    print(\"Answer:\\n\", result.answer)\n",
    "    query_tracker.print_summary(query_text, retriever_name, result.answer)\n",
    "\n",
    "# Run across all retrievers\n",
    "run_and_log_with_context(\"VectorRetriever\", vector_retriever, rag_template)\n",
    "run_and_log_with_context(\"VectorCypherRetriever\", vc_retriever, rag_template)\n",
    "run_and_log_with_context(\"HybridRetriever\", Hybrid_Retriever, rag_template)\n",
    "run_and_log_with_context(\"HybridCypherRetriever\", HybridCypher_Retriever, rag_template)\n",
    "run_and_log_with_context(\"Text2CypherRetriever\", Text2Cypher_Retriever, rag_template)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
