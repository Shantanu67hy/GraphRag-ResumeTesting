{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06890560",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neo4j-graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c839a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neo4j-graphrag[openai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd527822",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffd289c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This example demonstrates how to use SimpleKGPipeline with automatic schema extraction\n",
    "from a PDF file. When no schema is provided to SimpleKGPipeline, automatic schema extraction\n",
    "is performed using the LLM.\n",
    "\n",
    "Note: This example requires an OpenAI API key to be set in the .env file.\n",
    "\"\"\"\n",
    "\n",
    "import neo4j\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.generation.graphrag import GraphRAG\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.embeddings import AzureOpenAIEmbeddings\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "from neo4j_graphrag.llm import AzureOpenAILLM\n",
    "from neo4j_graphrag.experimental.components.text_splitters.fixed_size_splitter import FixedSizeSplitter\n",
    "from neo4j_graphrag.experimental.pipeline import Pipeline\n",
    "from neo4j_graphrag.schema import get_schema\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ef81432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.generation.graphrag import GraphRAG\n",
    "\n",
    "\n",
    "async def run_kg_pipeline_with_auto_schema() -> None:\n",
    "    \"\"\"Run the SimpleKGPipeline with automatic schema extraction from a PDF file.\"\"\"\n",
    "\n",
    "    # Define Neo4j connection\n",
    "NEO4J_URI=\"neo4j+ssc://efea2c90.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"7V18VY7NXa1QQl06JD7_FONhdeqSap_7pUMBTgg-o3A\"\n",
    "NEO4J_DATABASE=\"neo4j\"\n",
    "AURA_INSTANCEID=\"efea2c90\"\n",
    "AURA_INSTANCENAME=\"Instance01\"\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    " \n",
    "AUTH = (NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "with GraphDatabase.driver(NEO4J_URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba595511",
   "metadata": {},
   "source": [
    "Reading all the pdf files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a75f2ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('C:/Users/shant/OneDrive/Desktop/GraphRAGresume/input/Abhishek_Nandgadkar.pdf')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_path = Path(r\"C:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\input\")\n",
    "\n",
    "# Collect all PDF files in that folder\n",
    "pdf_files = list(folder_path.glob(\"*.pdf\"))\n",
    "print(pdf_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9c134ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: C:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\input\\Abhishek_Nandgadkar.pdf\n",
      "Extracted Text length: 2902\n",
      "Number of chunks: 1\n",
      "\n",
      "Chunk 1 (length 2901):\n",
      "Abhishek Nandgadkar\n",
      "Data Engineer | Data Analyst | SQL | Python | Spark | Airflow | ADF | Big Data | Real-Time Data Processing | Kafka\n",
      "| Cloud Services (AWS, GCP, Azure) | Building High-Performance ETL Pipelines\n",
      "abhisheknandgadkar41@gmail.com 6360952485 Belgaum, Karnataka\n",
      "abhishek-nandgadkar-4760a429b\n",
      "WORK EXPERIENCE\n",
      "Nubax Data Labs\n",
      "Data Engineer 02/2023 - Present\n",
      "• Designed scalable ETL pipelines for large datasets, optimizing performance.\n",
      "• Built efficient, reliable data workflows using Spark and AWS Glue, leveraging Glue jobs for schema\n",
      "management and batch data processing.\n",
      "• Utilized Databricks for distributed data processing, enabling scalable, high-performance analytics and\n",
      "reducing processing times for complex data transformations.\n",
      "• Conducted data profiling and validation to meet data governance and quality standards.\n",
      "• Implemented CI/CD pipelines for data workflows, reducing deployment times and improving reliability.\n",
      "• Created and maintained Grafana dashboards for real-time m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to extract full text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    full_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            full_text += page.extract_text() + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "# Folder containing PDF files\n",
    "folder_path = \"C:\\\\Users\\\\shant\\\\OneDrive\\\\Desktop\\\\GraphRAGresume\\\\input\"\n",
    "\n",
    "# List all PDF files in the folder\n",
    "pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "# Initialize RecursiveCharacterTextSplitter once\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"\\nProcessing file: {pdf_path}\")\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    print(\"Extracted Text length:\", len(full_text))\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    for i, chunk in enumerate(chunks[:5], 1):\n",
    "        print(f\"\\nChunk {i} (length {len(chunk)}):\\n{chunk[:1000]}\")  # Display first 1000 chars of chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09980214",
   "metadata": {},
   "source": [
    "Initialising LLM ,Embedder and Driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c491766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.generation.graphrag import GraphRAG\n",
    "\n",
    "# Neo4j connection details\n",
    "NEO4J_URI = \"neo4j+ssc://efea2c90.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"7V18VY7NXa1QQl06JD7_FONhdeqSap_7pUMBTgg-o3A\"\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6cf049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\.venv\\Lib\\site-packages\\neo4j\\_sync\\driver.py:542: ResourceWarning: unclosed  Neo4jDriver: <neo4j._sync.driver.Neo4jDriver object at 0x00000139662A2780>.\n",
      "  _unclosed_resource_warn(self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\.venv\\Lib\\site-packages\\neo4j\\_sync\\driver.py:547: DeprecationWarning: Relying on Driver's destructor to close the session is deprecated. Please make sure to close the session. Use it as a context (`with` statement) or make sure to call `.close()` explicitly. Future versions of the driver will not close drivers automatically.\n",
      "  _deprecation_warn(\n"
     ]
    }
   ],
   "source": [
    "    # Define LLM parameters\n",
    "\n",
    "llm_model_params = {\n",
    "        \"max_tokens\": 5000,\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"temperature\": 0,  # Lower temperature for more consistent output\n",
    "    }\n",
    "\n",
    "    # Initialize the Neo4j driver\n",
    "driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "    # Create the LLM instance\n",
    "\n",
    "llm = AzureOpenAILLM(\n",
    "    #model_name=\"gpt-4o-intern\",\n",
    "    # model_name=\"gpt-4o\",  # This should match your Azure deployment name for the LLM\n",
    "    #model_name =  # This should match your Azure deployment name for the LLM\n",
    "    #azure_endpoint=  # update with your endpoint\n",
    "    #api_version=  # update appropriate version\n",
    "    #api_key=  # api_key is optional and can also be set with OPENAI_API_KEY env var\n",
    "    #model_params=llm_model_params,\n",
    "    )\n",
    "\n",
    "    # Create the embedder instance\n",
    "\n",
    "embedder = AzureOpenAIEmbeddings(\n",
    "        #model=\n",
    "        # This should match your Azure deployment name for the embedding model\n",
    "        #api_key=  # Your Azure OpenAI API key\n",
    "        #azure_endpoint=  # update with your endpoint\n",
    "        #api_version=  # update appropriate version\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9027c14",
   "metadata": {},
   "source": [
    "Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd6b9866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\.venv\\Lib\\site-packages\\neo4j\\_sync\\driver.py:542: ResourceWarning: unclosed  Neo4jDriver: <neo4j._sync.driver.Neo4jDriver object at 0x0000013964E4E5A0>.\n",
      "  _unclosed_resource_warn(self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\shant\\OneDrive\\Desktop\\GraphRAGresume\\input\\Abhishek_Nandgadkar.pdf\n",
      "Result: run_id='7dd1bfee-da2c-4bee-820b-7548f193a425' result={'resolver': {'number_of_nodes_to_resolve': 27, 'number_of_created_nodes': 14}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "        # Create a SimpleKGPipeline instance without providing a schema\n",
    "        # This will trigger automatic schema extraction\n",
    "kg_builder = SimpleKGPipeline(\n",
    "llm=llm,\n",
    "driver=driver,\n",
    "embedder=embedder,\n",
    "from_pdf=True,\n",
    ")\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "    pdf_result=await kg_builder.run_async(file_path=str(pdf_file))\n",
    "    print(f\"Result: {pdf_result}\")\n",
    "    # Close connections\n",
    "#await llm.async_client.close()\n",
    "#driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def main() -> None:\n",
    "    # Run the pipeline\n",
    "    await run_kg_pipeline_with_auto_schema()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    await(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dde16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI=\"neo4j+ssc://efea2c90.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"7V18VY7NXa1QQl06JD7_FONhdeqSap_7pUMBTgg-o3A\"\n",
    "NEO4J_DATABASE=\"neo4j\"\n",
    "AURA_INSTANCEID=\"efea2c90\"\n",
    "AURA_INSTANCENAME=\"Instance01\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "captured_cypher_queries = []\n",
    "\n",
    "# Wrap the run method of session to capture queries\n",
    "original_session_run = driver.session().run\n",
    "\n",
    "def wrapped_run(self, query, parameters=None, **kwargs):\n",
    "    captured_cypher_queries.append(query)\n",
    "    return original_session_run(query, parameters or {}, **kwargs)\n",
    "\n",
    "# Patch the session run method\n",
    "from types import MethodType\n",
    "\n",
    "def patch_driver_run(driver):\n",
    "    original_session = driver.session\n",
    "\n",
    "    def new_session(*args, **kwargs):\n",
    "        session = original_session(*args, **kwargs)\n",
    "        session.run = MethodType(wrapped_run, session)\n",
    "        return session\n",
    "\n",
    "    driver.session = new_session\n",
    "\n",
    "patch_driver_run(driver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77b977",
   "metadata": {},
   "source": [
    "Creation of Indexes in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "\n",
    "INDEX_NAME = \"vector-index-name\"\n",
    "NEO4J_URI = \"neo4j+ssc://efea2c90.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"7V18VY7NXa1QQl06JD7_FONhdeqSap_7pUMBTgg-o3A\"\n",
    "\n",
    "# Connect to the Neo4j database\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "# Create the index (None return is normal)\n",
    "res = create_vector_index(\n",
    "    driver,\n",
    "    INDEX_NAME,\n",
    "    label=\"Chunk\",\n",
    "    embedding_property=\"embedding\",\n",
    "    dimensions=1536,\n",
    "    similarity_fn=\"cosine\",\n",
    ")\n",
    "\n",
    "print(f\"Index creation function returned: {res}\")\n",
    "\n",
    "# IMPORTANT: Verify if index was actually created\n",
    "print(\"🔍 Verifying index creation...\")\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"SHOW INDEXES WHERE name = $name\", name=INDEX_NAME)\n",
    "    index_info = result.single()\n",
    "    \n",
    "    if index_info:\n",
    "        details = dict(index_info)\n",
    "        print(f\"✅ SUCCESS! Index '{INDEX_NAME}' exists:\")\n",
    "        print(f\"   Type: {details.get('type')}\")\n",
    "        print(f\"   State: {details.get('state')}\")\n",
    "        print(f\"   Properties: {details.get('properties')}\")\n",
    "    else:\n",
    "        print(f\"❌ Index '{INDEX_NAME}' was not created\")\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.schema import get_schema\n",
    "from neo4j import GraphDatabase\n",
    " \n",
    "schema = get_schema(driver, database=\"neo4j\")\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa67a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    \n",
    "    entities = session.run(\"MATCH (n) RETURN count(n) as entity_count\").single()[\"entity_count\"]\n",
    "    relationships = session.run(\"MATCH ()-[r]->() RETURN count(r) as rel_count\").single()[\"rel_count\"]\n",
    "    \n",
    "    print(f\"Total entities: {entities}, Total relationships: {relationships}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cf9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_query = \"\"\"\n",
    "MATCH (n)\n",
    "WHERE NOT n:Document AND NOT n:Chunk\n",
    "WITH collect(n) AS nodes\n",
    "UNWIND nodes AS n\n",
    "MATCH p=(n)-[r]-(m)\n",
    "WHERE NOT m:Document AND NOT m:Chunk\n",
    "WITH DISTINCT p, labels(n) AS labels\n",
    "RETURN p,\n",
    "CASE \n",
    "    WHEN 'Person' IN labels THEN '#ff6b6b'       // red\n",
    "    WHEN 'Company' IN labels THEN '#4dabf7'      // blue\n",
    "    WHEN 'Location' IN labels THEN '#51cf66'     // green\n",
    "    WHEN 'Skill' IN labels THEN '#f59f00'        // orange\n",
    "    ELSE '#cccccc'                               // gray default\n",
    "END AS color\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    results = session.run(cypher_query)\n",
    "    for record in results:\n",
    "        path = record[\"p\"]\n",
    "        # path is a Path object: nodes and relationships can be extracted\n",
    "        nodes = [dict(node) for node in path.nodes]\n",
    "        relationships = [dict(rel) for rel in path.relationships]\n",
    "        print(f\"Path nodes: {nodes}\")\n",
    "        print(f\"Path relationships: {relationships}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03856011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "import tiktoken\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    " \n",
    "# # --- configure ---\n",
    "# NEO4J_URI = \"bolt://localhost:7687\"\n",
    "# NEO4J_USER = \"neo4j\"\n",
    "# NEO4J_PASSWORD = \"password\"\n",
    " \n",
    "# Choose tokenizer/model\n",
    "#MODEL_NAME = \"gpt-4o-mini\"  \n",
    "try:\n",
    "    enc = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "except KeyError:\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    " \n",
    "def my_converter(o):\n",
    "    if isinstance(o, datetime):\n",
    "        return o.isoformat()\n",
    "    # Add similar check for neo4j.time.DateTime or custom types as needed\n",
    "    return str(o)\n",
    "\n",
    "def serialize_value(v):\n",
    "    \"\"\"Stable JSON-like serialization for property values.\"\"\"\n",
    "    return json.dumps(v, ensure_ascii=True, sort_keys=True, default=my_converter)\n",
    "\n",
    "    #return json.dumps(v, ensure_ascii=True, sort_keys=True)\n",
    " \n",
    "def node_to_text(node):\n",
    "    \"\"\"Stable string for a node: labels + id + properties.\"\"\"\n",
    "    labels = \":\".join(sorted(list(node.labels))) if hasattr(node, \"labels\") else \"\"\n",
    "    props = dict(node.items())\n",
    "    prop_str = \", \".join(f'{k}={serialize_value(props[k])}' for k in sorted(props.keys()))\n",
    "    elem_id = getattr(node, \"element_id\", None) or getattr(node, \"id\", None)\n",
    "    return f\"({labels})[{elem_id}] {{{prop_str}}}\"\n",
    " \n",
    " \n",
    "def rel_to_text(rel):\n",
    "    \"\"\"Stable string for a relationship: type + ids + properties.\"\"\"\n",
    "    rel_type = rel.type if hasattr(rel, \"type\") else \"\"\n",
    "    props = dict(rel.items())\n",
    "    prop_str = \", \".join(f'{k}={serialize_value(props[k])}' for k in sorted(props.keys()))\n",
    "    start_id = getattr(rel, \"start_node_element_id\", None) or getattr(rel, \"start\", None)\n",
    "    end_id = getattr(rel, \"end_node_element_id\", None) or getattr(rel, \"end\", None)\n",
    "    elem_id = getattr(rel, \"element_id\", None) or getattr(rel, \"id\", None)\n",
    "    return f\"[:{rel_type}][{elem_id}] (start={start_id}, end={end_id}) {{{prop_str}}}\"\n",
    " \n",
    " \n",
    "def count_tokens(text):\n",
    "    return len(enc.encode(text))\n",
    " \n",
    " \n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(AUTH))\n",
    " \n",
    "# ---- NODES ----\n",
    "total_node_tokens = 0\n",
    "total_nodes = 0\n",
    "by_label_tokens = defaultdict(int)\n",
    "by_label_count = defaultdict(int)\n",
    " \n",
    "with driver.session() as session:\n",
    "    results = session.run(\"MATCH (n) RETURN n\")\n",
    "    for record in results:\n",
    "        n = record[\"n\"]\n",
    "        text = node_to_text(n)\n",
    "        tokens = count_tokens(text)\n",
    " \n",
    "        total_node_tokens += tokens\n",
    "        total_nodes += 1\n",
    " \n",
    "        label_key = tuple(sorted(getattr(n, \"labels\", ())))\n",
    "        by_label_tokens[label_key] += tokens\n",
    "        by_label_count[label_key] += 1\n",
    " \n",
    "# ---- RELATIONSHIPS ----\n",
    "total_rel_tokens = 0\n",
    "total_rels = 0\n",
    "by_type_tokens = defaultdict(int)\n",
    "by_type_count = defaultdict(int)\n",
    " \n",
    "with driver.session() as session:\n",
    "    results = session.run(\"MATCH ()-[r]->() RETURN r\")\n",
    "    for record in results:\n",
    "        r = record[\"r\"]\n",
    "        text = rel_to_text(r)\n",
    "        tokens = count_tokens(text)\n",
    " \n",
    "        total_rel_tokens += tokens\n",
    "        total_rels += 1\n",
    " \n",
    "        rel_type = r.type if hasattr(r, \"type\") else \"UNKNOWN\"\n",
    "        by_type_tokens[rel_type] += tokens\n",
    "        by_type_count[rel_type] += 1\n",
    " \n",
    "driver.close()\n",
    " \n",
    "# ---- Print Summary ----\n",
    "print(\"=== Nodes ===\")\n",
    "print(f\"Total nodes: {total_nodes}\")\n",
    "print(f\"Total node tokens: {total_node_tokens}\")\n",
    "for labels, tok in sorted(by_label_tokens.items(), key=lambda x: -x[1]):\n",
    "    label_name = \":\".join(labels) if labels else \"(no labels)\"\n",
    "    avg = tok / by_label_count[labels]\n",
    "    print(f\"- {label_name}: nodes={by_label_count[labels]}, tokens={tok}, avg_tokens_per_node={avg:.2f}\")\n",
    " \n",
    "print(\"\\n=== Relationships ===\")\n",
    "print(f\"Total relationships: {total_rels}\")\n",
    "print(f\"Total relationship tokens: {total_rel_tokens}\")\n",
    "for rel_type, tok in sorted(by_type_tokens.items(), key=lambda x: -x[1]):\n",
    "    avg = tok / by_type_count[rel_type]\n",
    "    print(f\"- {rel_type}: rels={by_type_count[rel_type]}, tokens={tok}, avg_tokens_per_rel={avg:.2f}\")\n",
    " \n",
    "print(\"\\n=== Overall ===\")\n",
    "print(f\"Total tokens (nodes + relationships): {total_node_tokens + total_rel_tokens}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89785267",
   "metadata": {},
   "source": [
    "token calculation for entities and relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6945d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_template = '''Answer the Question using the following Context. Only respond with information mentioned in the Context. Do not inject any speculative information not mentioned.\n",
    "\n",
    "# Question:\n",
    "{query_text}\n",
    "\n",
    "# Context:\n",
    "{context}\n",
    "\n",
    "# Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a3d71",
   "metadata": {},
   "source": [
    "vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "\n",
    "create_vector_index(driver, \n",
    "                    name=\"vector-index-name\", \n",
    "                    label=\"Chunk\", \n",
    "                    embedding_property=\"embedding\", \n",
    "                    dimensions=1536,  \n",
    "                    similarity_fn=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "\n",
    "vector_retriever = VectorRetriever(\n",
    "   driver,\n",
    "   index_name=\"vector-index-name\",\n",
    "   embedder=embedder,\n",
    "   return_properties=[\"text\", \"chunk_text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "vector_res = vector_retriever.get_search_results(query_text = \"Who all have worked with vector Databases?\",top_k=5)\n",
    "for i in vector_res.records: print(\"====n\" + json.dumps(i.data(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e679b3a",
   "metadata": {},
   "source": [
    "VectorCypherRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa39e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorCypherRetriever\n",
    "\n",
    "vc_retriever = VectorCypherRetriever(\n",
    "    driver,\n",
    "    index_name=\"vector-index-name\",\n",
    "    embedder=embedder,\n",
    "    retrieval_query=\"\"\"\n",
    "//1) Go out 2-3 hops in the entity graph and get relationships\n",
    "\n",
    "WITH node AS chunk\n",
    "MATCH (chunk)<-[:FROM_CHUNK]-()-[relList:!FROM_CHUNK]-{1,2}()\n",
    "UNWIND relList AS rel\n",
    " \n",
    "//2) collect relationships and text chunks\n",
    "WITH collect(DISTINCT chunk) AS chunks,\n",
    " collect(DISTINCT rel) AS rels\n",
    " \n",
    "//3) format and return context\n",
    "RETURN '=== text ===n' + apoc.text.join([c in chunks | c.text], 'n---n') + 'nn=== kg_rels ===n' +\n",
    " apoc.text.join([r in rels | startNode(r).name + ' - ' + type(r) + '(' + coalesce(r.details, '') + ')' +  ' -> ' + endNode(r).name ], 'n---n') AS info\n",
    "\"\"\"\n",
    ")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_res = vc_retriever.get_search_results(query_text = \"summarise me the resume of ALOK MISHRA?\", top_k=1)\n",
    "\n",
    "# print output\n",
    "kg_rel_pos = vc_res.records[0]['info'].find('nn=== kg_rels ===n')\n",
    "print(\"# Text Chunk Context:\")\n",
    "print(vc_res.records[0]['info'][:kg_rel_pos])\n",
    "print(\"# KG Context From Relationships:\")\n",
    "print(vc_res.records[0]['info'][kg_rel_pos:])\n",
    "\n",
    "# Assume vector_retriever and embedder are already initialized as shown earlier\n",
    "\n",
    "# Embed the query into a vector\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962552c",
   "metadata": {},
   "source": [
    "Text2Cypher Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j\n",
    "from neo4j_graphrag.retrievers import Text2CypherRetriever\n",
    "\n",
    "\n",
    "llm_model_params = {\n",
    "        \"max_tokens\": 2000,\n",
    "        \"response_format\": {\"type\": \"text\"},\n",
    "        \"temperature\": 0,  # Lower temperature for more consistent output\n",
    "    }\n",
    "\n",
    "    # Initialize the Neo4j driver\n",
    "driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "llm = AzureOpenAILLM(\n",
    "    #model_name=\"gpt-4o-intern\",\n",
    "    # model_name=\"gpt-4o\",  # This should match your Azure deployment name for the LLM\n",
    "# (Optional) Provide user input/query pairs for the LLM to use as examples\n",
    ")\n",
    "\n",
    "with neo4j.GraphDatabase.driver(NEO4J_URI, auth=AUTH) as driver:\n",
    "    # Initialize the retriever\n",
    "    Text2Cypher_Retriever = Text2CypherRetriever(\n",
    "        driver,\n",
    "        llm=llm,\n",
    "        neo4j_schema=schema,\n",
    "        neo4j_database='neo4j',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e78001",
   "metadata": {},
   "source": [
    "Hybrid-Cypher Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1604e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_fulltext_index\n",
    "FULLTEXT_INDEX_NAME = \"fulltext_index\"\n",
    "\n",
    "driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=AUTH)\n",
    "create_fulltext_index(\n",
    "    driver, FULLTEXT_INDEX_NAME, label=\"Document\", node_properties=[\"textProperty\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d31cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import HybridCypherRetriever\n",
    "llm_model_params = {\n",
    "        \"max_tokens\": 2000,\n",
    "        \"response_format\": {\"type\": \"text\"},\n",
    "        \"temperature\": 0,  # Lower temperature for more consistent output\n",
    "    }\n",
    "llm = AzureOpenAILLM(\n",
    "    #model_name=\"gpt-4o-intern\",\n",
    "    # model_name=\"gpt-4o\",  # This should match your Azure deployment name for the LLM\n",
    ")\n",
    "retrieval_query = \"\"\"\n",
    "// Just return the main content without complex relationships\n",
    "WITH node\n",
    "RETURN \n",
    "    CASE \n",
    "        WHEN node.text IS NOT NULL THEN node.text\n",
    "        WHEN node.content IS NOT NULL THEN node.content  \n",
    "        WHEN node.description IS NOT NULL THEN node.description\n",
    "        WHEN node.name IS NOT NULL THEN 'Name: ' + node.name\n",
    "        ELSE 'Node ID: ' + elementId(node)\n",
    "    END AS info\n",
    "\"\"\"\n",
    "\n",
    "with neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "    # Initialize the retriever\n",
    "    HybridCypher_Retriever = HybridCypherRetriever(\n",
    "        driver=driver,\n",
    "        vector_index_name='vector-index-name',\n",
    "        fulltext_index_name='fulltext_index',\n",
    "        embedder=embedder,\n",
    "        retrieval_query=retrieval_query,\n",
    "        result_formatter=None,\n",
    "    )\n",
    "    # Perform the similarity search for a text query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e09b00",
   "metadata": {},
   "source": [
    "Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import HybridRetriever\n",
    "\n",
    "INDEX_NAME = \"vector-index-name\"\n",
    "FULLTEXT_INDEX_NAME = \"fulltext_index\"\n",
    "\n",
    "llm_model_params = {\n",
    "        \"max_tokens\": 2000,\n",
    "        #\"response_format\": {\"type\": \"json_object\"},\n",
    "        \"temperature\": 0,  # Lower temperature for more consistent output\n",
    "    }\n",
    "\n",
    "    # Initialize the Neo4j driver\n",
    "    # Create the LLM instance\n",
    "\n",
    "llm = AzureOpenAILLM(\n",
    "    #model_name=\"gpt-4o-intern\",\n",
    "    # model_name=\"gpt-4o\",  # This should match your Azure deployment name for the LLM\n",
    "    \n",
    "    )\n",
    "with neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "    # Initialize the retriever\n",
    "    Hybrid_Retriever = HybridRetriever(\n",
    "        driver=driver,\n",
    "        vector_index_name=\"vector-index-name\",\n",
    "        fulltext_index_name=\"fulltext_index\",\n",
    "        embedder=embedder,\n",
    "    )\n",
    "\n",
    "    # Perform the similarity search for a text query\n",
    "    # (retrieve the top 5 most similar nodes)\n",
    "    #query_text = \"Who all have worked with vector Databases\"\n",
    "    #print(Hybrid_Retriever.search(query_text=query_text, top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4fbbf",
   "metadata": {},
   "source": [
    "Instantiate the rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.generation.prompts import RagTemplate\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "\n",
    "# Define the template as a string\n",
    "template_str = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query_text}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the RagTemplate object\n",
    "rag_template = RagTemplate(\n",
    "    template=template_str,\n",
    "    expected_inputs=['query_text', 'context']\n",
    ")\n",
    "\n",
    "\n",
    "vector_retriever_rag  = GraphRAG(llm=llm, retriever=vector_retriever, prompt_template=rag_template)\n",
    "vector_cypher_retriever_rag = GraphRAG(llm=llm, retriever=vc_retriever, prompt_template=rag_template)\n",
    "Text2Cypher_Retriever_rag = GraphRAG(retriever=Text2Cypher_Retriever, llm=llm)\n",
    "HybridCypher_Retriever_rag = GraphRAG(retriever=HybridCypher_Retriever, llm=llm)\n",
    "Hybrid_Retriever_rag = GraphRAG(retriever=Hybrid_Retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0a670",
   "metadata": {},
   "source": [
    "Different Query Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveQueryTracker:\n",
    "    def __init__(self, driver):\n",
    "        self.driver = driver\n",
    "        self.tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "        self.reset_sessions()\n",
    "    \n",
    "    def reset_sessions(self):\n",
    "        self.query_tokens = 0\n",
    "        self.embedding_tokens = 0\n",
    "        self.retrieval_prompt_tokens = 0\n",
    "        self.retrieval_completion_tokens = 0\n",
    "        self.context_tokens = 0\n",
    "        self.final_answer_tokens = 0\n",
    "        self.start_time = datetime.utcnow()\n",
    "    \n",
    "    def count_tokens(self, text):\n",
    "        return len(self.tokenizer.encode(text or \"\"))\n",
    "    \n",
    "    def track_embedding(self, text):\n",
    "        tokens = self.count_tokens(text)\n",
    "        self.embedding_tokens += tokens\n",
    "        print(f\"🔍 [QUERY EMBED] {tokens} tokens\")\n",
    "        return tokens\n",
    "    \n",
    "    def track_context(self, context):\n",
    "        tokens = self.count_tokens(context)\n",
    "        self.context_tokens += tokens\n",
    "        print(\"\\n📄 [RETRIEVED CONTEXT]\")\n",
    "        print(\"-\"*80)\n",
    "        print(context[:2000] + (\"...\" if len(context) > 2000 else \"\"))\n",
    "        print(\"-\"*80)\n",
    "        return tokens\n",
    "    \n",
    "    def track_llm(self, prompt, response):\n",
    "        prompt_tokens = self.count_tokens(prompt)\n",
    "        completion_tokens = 0\n",
    "        if isinstance(response, dict):\n",
    "            completion_tokens = response.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "            if completion_tokens == 0 and response.get(\"choices\"):\n",
    "                text_resp = response[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\") or response[\"choices\"][0].get(\"text\", \"\")\n",
    "                completion_tokens = self.count_tokens(text_resp)\n",
    "        elif isinstance(response, str):\n",
    "            completion_tokens = self.count_tokens(response)\n",
    "        elif hasattr(response, \"text\"):\n",
    "            completion_tokens = self.count_tokens(response.text)\n",
    "        else:\n",
    "            completion_tokens = self.count_tokens(str(response))\n",
    "        \n",
    "        self.retrieval_prompt_tokens += prompt_tokens\n",
    "        self.retrieval_completion_tokens += completion_tokens\n",
    "        print(f\"🤖 [LLM] prompt={prompt_tokens}, completion={completion_tokens}\")\n",
    "        return prompt_tokens, completion_tokens\n",
    "    \n",
    "    def print_summary(self, query, retriever_name, answer):\n",
    "        self.query_tokens = self.count_tokens(query)\n",
    "        self.final_answer_tokens = self.count_tokens(answer)\n",
    "        total_input = (\n",
    "            self.query_tokens +\n",
    "            self.embedding_tokens +\n",
    "            self.retrieval_prompt_tokens +\n",
    "            self.context_tokens\n",
    "        )\n",
    "        total_output = self.retrieval_completion_tokens + self.final_answer_tokens\n",
    "        grand_total = total_input + total_output\n",
    "        duration = (datetime.utcnow() - self.start_time).total_seconds()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"📊 QUERY TOKEN SUMMARY for {retriever_name}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Query tokens:           {self.query_tokens}\")\n",
    "        print(f\"Embedding tokens:       {self.embedding_tokens}\")\n",
    "        #print(f\"Retrieval LLM prompt:   {self.retrieval_prompt_tokens}\")\n",
    "        #print(f\"Retrieval LLM output:   {self.retrieval_completion_tokens}\")\n",
    "        print(f\"Context tokens:         {self.context_tokens}\")\n",
    "        print(f\"Final answer tokens:    {self.final_answer_tokens}\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Total input tokens:     {total_input}\")\n",
    "        print(f\"Total output tokens:    {total_output}\")\n",
    "        print(f\"Grand total tokens:     {grand_total}\")\n",
    "        print(f\"Duration (s):           {duration:.2f}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "query_tracker = ComprehensiveQueryTracker(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"give me education details of Abhishek Nandgadkar?\"\n",
    "\n",
    "def run_and_log_with_context(retriever_name, retriever, prompt_template=None):\n",
    "    query_tracker.reset_sessions()\n",
    "\n",
    "    # 1) Raw context retrieval (skip for Text2Cypher)\n",
    "    if hasattr(retriever, \"get_search_results\") and retriever_name != \"Text2CypherRetriever\":\n",
    "        try:\n",
    "            raw = retriever.get_search_results(query_text=query_text, top_k=5)\n",
    "            print(f\"🔍 Retrieved {len(raw.records)} records from {retriever_name}\")\n",
    "            texts = []\n",
    "            for rec in raw.records:\n",
    "                data = rec.data()\n",
    "                if \"node\" in data and isinstance(data[\"node\"], dict) and \"text\" in data[\"node\"]:\n",
    "                    texts.append(data[\"node\"][\"text\"])\n",
    "                else:\n",
    "                    for v in data.values():\n",
    "                        if isinstance(v, str) and v.strip():\n",
    "                            texts.append(v)\n",
    "            \n",
    "            if texts:\n",
    "                context = \"\\n---\\n\".join(texts)\n",
    "                \n",
    "                # PRINT THE ACTUAL CONTEXT BEING TOKENIZED\n",
    "                print(f\"\\n📄 CONTEXT CONTENT for {retriever_name}:\")\n",
    "                print(\"=\" * 80)\n",
    "                print(context[:2000] + \"...\" if len(context) > 2000 else context)  # Show first 2000 chars\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"Full context length: {len(context)} characters\")\n",
    "                \n",
    "                query_tracker.track_context(context)\n",
    "            else:\n",
    "                print(f\"⚠️ No context extracted for {retriever_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Context fetch failed for {retriever_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"ℹ️ Skipping raw context fetch for {retriever_name}\")\n",
    "\n",
    "    # 2) Perform RAG search\n",
    "    rag = (\n",
    "        GraphRAG(llm=llm, retriever=retriever, prompt_template=prompt_template)\n",
    "        if prompt_template\n",
    "        else GraphRAG(llm=llm, retriever=retriever)\n",
    "    )\n",
    "    \n",
    "    if retriever_name == \"Text2CypherRetriever\":\n",
    "        result = rag.search(query_text)\n",
    "    else:\n",
    "        result = rag.search(query_text, retriever_config={\"top_k\": 5})\n",
    "\n",
    "    # 3) Print answer and tokens summary\n",
    "    print(f\"\\n===== {retriever_name} =====\")\n",
    "    print(\"Answer:\\n\", result.answer)\n",
    "    query_tracker.print_summary(query_text, retriever_name, result.answer)\n",
    "\n",
    "# Run across all retrievers\n",
    "run_and_log_with_context(\"VectorRetriever\", vector_retriever, rag_template)\n",
    "run_and_log_with_context(\"VectorCypherRetriever\", vc_retriever, rag_template)\n",
    "run_and_log_with_context(\"HybridRetriever\", Hybrid_Retriever, rag_template)\n",
    "run_and_log_with_context(\"HybridCypherRetriever\", HybridCypher_Retriever, rag_template)\n",
    "run_and_log_with_context(\"Text2CypherRetriever\", Text2Cypher_Retriever, rag_template)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
